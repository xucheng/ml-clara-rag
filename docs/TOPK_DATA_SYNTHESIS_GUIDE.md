# CLaRa Top-K Data Synthesis Guide

è¿™ä»½æŒ‡å—è¯´æ˜å¦‚ä½•ä¿®æ”¹ data pipeline æ¥ç”Ÿæˆæ”¯æŒ `generation_top_k > 1` çš„è®­ç»ƒæ•°æ®ã€‚

## é—®é¢˜è¯Šæ–­

### å½“å‰é—®é¢˜

åŸå§‹çš„ `scripts/synthesize_data.py` åœ¨ **Line 151** åªä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆ 1 ä¸ªæ–‡æ¡£ï¼š

```python
qa_entry = {
    "question": qa.get("question"),
    "docs": [chunk],  # âŒ åªæœ‰ 1 ä¸ªæ–‡æ¡£
    "gold_answer": qa.get("answer")
}
```

è¿™å¯¼è‡´ï¼š
- `generation_top_k=5` æ—¶ä¼šæŠ¥é”™ï¼š`RuntimeError: selected index k out of range`
- å³ä½¿è®¾ç½® `generation_top_k=1`ï¼Œæ¨¡å‹ä¹Ÿæ— æ³•å­¦ä¹ æ–‡æ¡£æ’åºå’Œå¤šæ–‡æ¡£èåˆèƒ½åŠ›

## è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šä½¿ç”¨æ–°è„šæœ¬ `synthesize_data_topk.py` (æ¨è)

æˆ‘åˆ›å»ºäº†ä¸€ä¸ªå¢å¼ºç‰ˆçš„æ•°æ®åˆæˆè„šæœ¬ï¼Œæ”¯æŒä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆå¤šä¸ªå€™é€‰æ–‡æ¡£ã€‚

#### ç‰¹æ€§

- âœ… **å¯é…ç½® top-k å€¼**ï¼šé€šè¿‡ `--target_top_k` å‚æ•°è®¾ç½®ï¼ˆ1-10ï¼‰
- âœ… **ä¸¤ç§è´Ÿæ ·æœ¬ç­–ç•¥**ï¼š
  - **éšæœºé‡‡æ ·**ï¼ˆé»˜è®¤ï¼‰ï¼šéšæœºé€‰æ‹©å…¶ä»–æ–‡æ¡£å—ä½œä¸ºè´Ÿæ ·æœ¬
  - **ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜**ï¼ˆä½¿ç”¨ `--use_embeddings`ï¼‰ï¼šåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦é€‰æ‹©æœ€å…·è¿·æƒ‘æ€§çš„è´Ÿæ ·æœ¬
- âœ… **è‡ªåŠ¨æ–‡æ¡£æ··æ’**ï¼šæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬éšæœºæ‰“ä¹±ï¼Œæ¨¡æ‹ŸçœŸå®æ£€ç´¢ç»“æœ
- âœ… **ä¿æŒå‘åå…¼å®¹**ï¼š`--target_top_k 1` æ—¶è¡Œä¸ºä¸åŸè„šæœ¬ç›¸åŒ

#### ä½¿ç”¨æ–¹æ³•

##### åŸºç¡€ç”¨æ³•ï¼ˆéšæœºè´Ÿæ ·æœ¬ï¼‰

```bash
# ç”Ÿæˆ top-k=5 çš„è®­ç»ƒæ•°æ®ï¼ˆéšæœºè´Ÿæ ·æœ¬ï¼‰
python scripts/synthesize_data_topk.py \
    --input_file example/raw_knowledge.jsonl \
    --output_dir example \
    --api_key $OPENAI_API_KEY \
    --base_url $BASE_URL \
    --model qwen-turbo \
    --target_top_k 5
```

**ä¼˜ç‚¹**ï¼š
- ğŸš€ é€Ÿåº¦å¿«ï¼ˆä¸éœ€è¦ç”Ÿæˆ embeddingsï¼‰
- ğŸ’° æˆæœ¬ä½ï¼ˆåªè°ƒç”¨ LLM APIï¼‰
- âœ… é€‚åˆæ–‡æ¡£æ•°é‡ > 20 çš„åœºæ™¯

**ç¼ºç‚¹**ï¼š
- è´Ÿæ ·æœ¬è´¨é‡ä¸€èˆ¬ï¼ˆå¯èƒ½å®Œå…¨æ— å…³ï¼‰
- æ¨¡å‹å¯èƒ½å­¦ä¼šç®€å•çš„è¡¨é¢ç‰¹å¾åŒºåˆ†

##### é«˜çº§ç”¨æ³•ï¼ˆç¡¬è´Ÿæ ·æœ¬æŒ–æ˜ï¼‰

```bash
# ç”Ÿæˆ top-k=5 çš„è®­ç»ƒæ•°æ®ï¼ˆåŸºäºåµŒå…¥çš„ç¡¬è´Ÿæ ·æœ¬ï¼‰
python scripts/synthesize_data_topk.py \
    --input_file example/raw_knowledge.jsonl \
    --output_dir example \
    --api_key $OPENAI_API_KEY \
    --base_url https://api.openai.com/v1 \
    --model qwen-turbo \
    --target_top_k 5 \
    --use_embeddings
```

**ä¼˜ç‚¹**ï¼š
- ğŸ¯ é«˜è´¨é‡è´Ÿæ ·æœ¬ï¼ˆè¯­ä¹‰ç›¸ä¼¼ä½†ä¸åŒ…å«ç­”æ¡ˆï¼‰
- ğŸ§  è®­ç»ƒæ›´å…·æŒ‘æˆ˜æ€§çš„æ£€ç´¢èƒ½åŠ›
- âœ… æ¨¡å‹å­¦ä¼šç»†ç²’åº¦çš„ç›¸å…³æ€§åˆ¤æ–­

**ç¼ºç‚¹**ï¼š
- ğŸŒ é€Ÿåº¦æ…¢ï¼ˆéœ€è¦ä¸ºæ¯ä¸ª chunk ç”Ÿæˆ embeddingï¼‰
- ğŸ’¸ æˆæœ¬é«˜ï¼ˆé¢å¤–çš„ embedding API è°ƒç”¨ï¼‰
- âš ï¸ éœ€è¦ OpenAI APIï¼ˆ`text-embedding-3-small` æ¨¡å‹ï¼‰

#### è¾“å‡ºç¤ºä¾‹

ç”Ÿæˆçš„ `end_to_end_data.jsonl` æ ¼å¼ï¼š

```json
{
  "question": "How do I prevent overfitting in neural networks?",
  "docs": [
    "Neural networks consist of layers...",  // Weakly relevant (negative)
    "Overfitting occurs when models memorize training data...",  // Relevant (positive)
    "Batch normalization can accelerate training...",  // Moderately relevant
    "Learning rate controls the step size...",  // Weakly relevant (negative)
    "Dropout randomly drops neurons to prevent overfitting..."  // Relevant (positive)
  ],
  "gold_answer": "Methods to prevent overfitting: 1) Use more training data, 2) Apply dropout..."
}
```

**æ³¨æ„**ï¼š
- `docs` æ•°ç»„é•¿åº¦ = `target_top_k`
- æ–‡æ¡£é¡ºåºæ˜¯éšæœºçš„ï¼ˆæ¨¡æ‹ŸçœŸå®æ£€ç´¢ç»“æœï¼‰
- è‡³å°‘åŒ…å« 1 ä¸ªæ­£æ ·æœ¬ï¼ˆèƒ½å›ç­”é—®é¢˜çš„æ–‡æ¡£ï¼‰

---

### æ–¹æ¡ˆ 2ï¼šä¿®æ”¹åŸå§‹è„šæœ¬ `synthesize_data.py`

å¦‚æœä¸æƒ³ä½¿ç”¨æ–°è„šæœ¬ï¼Œå¯ä»¥æ‰‹åŠ¨ä¿®æ”¹ `synthesize_data.py`ï¼š

#### ä¿®æ”¹æ­¥éª¤

1. **åœ¨ä¸»å‡½æ•°å¼€å¤´æ·»åŠ å‚æ•°**ï¼š

```python
def main():
    args = parse_args()

    TARGET_TOP_K = 5  # âœ… æ·»åŠ è¿™ä¸€è¡Œ

    # ... rest of code
```

2. **ä¿®æ”¹ Line 145-157**ï¼š

```python
# OLD CODE (åªç”Ÿæˆ 1 ä¸ªæ–‡æ¡£)
for qa in data["qa_pairs"]:
    qa_entry = {
        "question": qa.get("question"),
        "docs": [chunk],  # âŒ å•æ–‡æ¡£
        "gold_answer": qa.get("answer")
    }
    f_instruct.write(json.dumps(qa_entry, ensure_ascii=False) + "\n")
    f_e2e.write(json.dumps(qa_entry, ensure_ascii=False) + "\n")
```

```python
# NEW CODE (ç”Ÿæˆå¤šä¸ªå€™é€‰æ–‡æ¡£)
for qa in data["qa_pairs"]:
    # Select negative documents
    negative_docs = []
    available_chunks = [c for j, c in enumerate(chunks) if j != i]

    if len(available_chunks) >= (TARGET_TOP_K - 1):
        negative_docs = random.sample(available_chunks, TARGET_TOP_K - 1)
    else:
        negative_docs = available_chunks  # Use all available if not enough

    # Combine positive + negatives and shuffle
    candidate_docs = [chunk] + negative_docs
    random.shuffle(candidate_docs)

    qa_entry = {
        "question": qa.get("question"),
        "docs": candidate_docs,  # âœ… å¤šæ–‡æ¡£
        "gold_answer": qa.get("answer")
    }
    f_instruct.write(json.dumps(qa_entry, ensure_ascii=False) + "\n")
    f_e2e.write(json.dumps(qa_entry, ensure_ascii=False) + "\n")
```

3. **æ·»åŠ  import**ï¼š

åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ ï¼š
```python
import random
```

---

## è®­ç»ƒé…ç½®è°ƒæ•´

ç”Ÿæˆ top-k=5 æ•°æ®åï¼Œéœ€è¦ç›¸åº”è°ƒæ•´è®­ç»ƒè„šæœ¬å‚æ•°ã€‚

### Colab Notebook ä¿®æ”¹

åœ¨ `training_colab_complete.ipynb` çš„ **Cell 30 (Stage 3 Training)** ä¸­ï¼š

```python
# Stage 3 é…ç½®
--stage stage2 \
--generation_top_k 5 \  # âœ… æ”¹ä¸º 5ï¼ˆæˆ–ä½ çš„ target_top_k å€¼ï¼‰
```

### æœ¬åœ°è®­ç»ƒè„šæœ¬ä¿®æ”¹

å¦‚æœä½¿ç”¨ `openrlhf/cli/train_sft.py`ï¼š

```bash
deepspeed --module openrlhf.cli.train_sft \
   --stage stage2 \
   --generation_top_k 5 \  # âœ… æ”¹ä¸º 5
   # ... other args
```

---

## æ•°æ®è´¨é‡è¦æ±‚

### Top-K=1ï¼ˆå•æ–‡æ¡£æ¨¡å¼ï¼‰

**æ¯ä¸ªæ ·æœ¬éœ€è¦**ï¼š
- 1 ä¸ªé«˜è´¨é‡æ–‡æ¡£ï¼ˆåŒ…å«ç­”æ¡ˆçš„å…¨éƒ¨ä¿¡æ¯ï¼‰

**æ•°æ®åˆæˆéš¾åº¦**ï¼šç®€å•
**é€‚ç”¨åœºæ™¯**ï¼šç®€å• QAã€å¿«é€ŸåŸå‹

### Top-K=5ï¼ˆå¤šæ–‡æ¡£æ¨¡å¼ï¼‰

**æ¯ä¸ªæ ·æœ¬éœ€è¦**ï¼š
- 1 ä¸ªæ ¸å¿ƒç›¸å…³æ–‡æ¡£ï¼ˆåŒ…å«ä¸»è¦ç­”æ¡ˆï¼‰
- 2-3 ä¸ªè¾…åŠ©ç›¸å…³æ–‡æ¡£ï¼ˆè¡¥å……ä¿¡æ¯ï¼‰
- 1-2 ä¸ªå›°éš¾è´Ÿæ ·æœ¬ï¼ˆä¸»é¢˜ç›¸å…³ä½†ä¸å›ç­”é—®é¢˜ï¼‰

**æ•°æ®åˆæˆéš¾åº¦**ï¼šä¸­ç­‰
**é€‚ç”¨åœºæ™¯**ï¼šå¤æ‚æ¨ç†ã€å¤šæ¥æºä¿¡æ¯æ•´åˆ

### æ¨èé…ç½®

| æ–‡æ¡£åº“è§„æ¨¡ | æ¨è Top-K | è´Ÿæ ·æœ¬ç­–ç•¥ | åŸå›  |
|-----------|-----------|-----------|------|
| < 10 chunks | 1-2 | éšæœº | æ–‡æ¡£ä¸å¤Ÿï¼Œå¼ºåˆ¶ top-k=5 ä¼šé‡å¤ |
| 10-50 chunks | 3-5 | éšæœº | è¶³å¤Ÿå¤šæ ·æ€§ |
| > 50 chunks | 5-10 | ç¡¬è´Ÿæ ·æœ¬ | å¯ä»¥æŒ–æ˜é«˜è´¨é‡è´Ÿæ ·æœ¬ |

---

## å®Œæ•´å·¥ä½œæµç¤ºä¾‹

### åœºæ™¯ï¼šä»é›¶å¼€å§‹ç”Ÿæˆ top-k=5 è®­ç»ƒæ•°æ®

```bash
# Step 1: æå–åŸå§‹çŸ¥è¯†ï¼ˆå‡è®¾ä½ æœ‰ PowerPoint æ–‡ä»¶ï¼‰
python scripts/extract_with_docling.py \
    --input_dir /path/to/pptx/files \
    --output_file example/raw_knowledge.jsonl

# Step 2: ä½¿ç”¨æ–°è„šæœ¬åˆæˆ top-k=5 æ•°æ®ï¼ˆç¡¬è´Ÿæ ·æœ¬ï¼‰
python scripts/synthesize_data_topk.py \
    --input_file example/raw_knowledge.jsonl \
    --output_dir example \
    --api_key $OPENAI_API_KEY \
    --base_url https://api.openai.com/v1 \
    --model gpt-4o-mini \
    --target_top_k 5 \
    --use_embeddings

# Step 3: æ£€æŸ¥ç”Ÿæˆçš„æ•°æ®
head -1 example/end_to_end_data.jsonl | python -m json.tool

# è¾“å‡ºåº”è¯¥ç±»ä¼¼ï¼š
# {
#   "question": "...",
#   "docs": ["doc1", "doc2", "doc3", "doc4", "doc5"],  // 5 ä¸ªæ–‡æ¡£
#   "gold_answer": "..."
# }

# Step 4: ä¸Šä¼ åˆ° Colab å¹¶è®­ç»ƒ
# åœ¨ Colab ä¸­è¿è¡Œ training_colab_complete.ipynb
# ç¡®ä¿ Stage 3 é…ç½®ä¸­ --generation_top_k 5
```

---

## å¸¸è§é—®é¢˜

### Q1: æ•°æ®ä¸­å·²ç»æœ‰å¤šä¸ªæ–‡æ¡£ï¼Œä½†è®­ç»ƒè¿˜æ˜¯æŠ¥é”™ï¼Ÿ

**A**: æ£€æŸ¥ä¸¤ç‚¹ï¼š
1. ç¡®è®¤ Colab ä¸­ä»£ç æ˜¯æœ€æ–°çš„ï¼ˆè¿è¡Œ `!git pull`ï¼‰
2. ç¡®è®¤ `--generation_top_k` å‚æ•°ä¸æ•°æ®ä¸­ `docs` æ•°ç»„é•¿åº¦ä¸€è‡´

### Q2: ä½¿ç”¨ `--use_embeddings` æ—¶æŠ¥é”™ï¼Ÿ

**A**: ç¡®ä¿ï¼š
- `--base_url` è®¾ç½®ä¸º `https://api.openai.com/v1`ï¼ˆOpenAI å®˜æ–¹ APIï¼‰
- API key æœ‰è®¿é—® `text-embedding-3-small` æ¨¡å‹çš„æƒé™
- å¦‚æœä½¿ç”¨å…¶ä»– API providerï¼ˆå¦‚ DashScopeï¼‰ï¼Œç§»é™¤ `--use_embeddings`

### Q3: è´Ÿæ ·æœ¬è´¨é‡ä¸å¥½ï¼Œæ€ä¹ˆåŠï¼Ÿ

**A**: ä¸‰ç§æ–¹æ³•ï¼š
1. ä½¿ç”¨ `--use_embeddings` å¯ç”¨ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜
2. å¢åŠ  `--chunk_size`ï¼ˆä¾‹å¦‚ 1500ï¼‰ï¼Œè®©æ¯ä¸ª chunk åŒ…å«æ›´å®Œæ•´çš„è¯­ä¹‰
3. æ‰‹åŠ¨æ„é€ è´Ÿæ ·æœ¬ï¼ˆç¼–è¾‘ç”Ÿæˆçš„ JSONL æ–‡ä»¶ï¼‰

### Q4: Top-K è®¾ç½®å¤šå¤§åˆé€‚ï¼Ÿ

**A**: æ ¹æ®åœºæ™¯ï¼š
- **å¿«é€ŸéªŒè¯**ï¼štop-k=1ï¼ˆå•æ–‡æ¡£ï¼‰
- **ä¸€èˆ¬åº”ç”¨**ï¼štop-k=3-5ï¼ˆå¹³è¡¡æ€§èƒ½å’Œæˆæœ¬ï¼‰
- **é«˜éš¾åº¦ä»»åŠ¡**ï¼štop-k=8-10ï¼ˆéœ€è¦å¤šæ–‡æ¡£èåˆï¼‰

### Q5: å¯ä»¥æ··åˆä½¿ç”¨ä¸åŒ top-k çš„æ•°æ®å—ï¼Ÿ

**A**: å¯ä»¥ï¼CLaRa çš„è‡ªåŠ¨è°ƒæ•´é€»è¾‘ï¼ˆcommit 1b99307ï¼‰ä¼šå¤„ç†ï¼š
```python
actual_top_k = min(self.generation_top_k, len(docs))
```
ä½†å»ºè®®ä¿æŒä¸€è‡´ä»¥æœ€å¤§åŒ–è®­ç»ƒæ•ˆæœã€‚

---

## æ€»ç»“

| ä¿®æ”¹å†…å®¹ | ä½ç½® | ä¿®æ”¹éš¾åº¦ |
|---------|------|---------|
| âœ… **ä½¿ç”¨æ–°è„šæœ¬** | `scripts/synthesize_data_topk.py` | ç®€å•ï¼ˆæ¨èï¼‰ |
| âš ï¸ **ä¿®æ”¹åŸè„šæœ¬** | `scripts/synthesize_data.py` L145-157 | ä¸­ç­‰ |
| âœ… **è°ƒæ•´è®­ç»ƒå‚æ•°** | `training_colab_complete.ipynb` Cell 30 | ç®€å• |

**æœ€ä½³å®è·µ**ï¼š
1. ä½¿ç”¨ `synthesize_data_topk.py --target_top_k 5 --use_embeddings`
2. ç¡®ä¿æ–‡æ¡£åº“ > 20 chunks
3. è®­ç»ƒæ—¶è®¾ç½® `--generation_top_k 5`
4. éªŒè¯æ•°æ®æ ¼å¼ï¼šæ¯ä¸ªæ ·æœ¬çš„ `docs` æ•°ç»„é•¿åº¦ = 5
