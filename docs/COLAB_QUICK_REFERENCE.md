# CLaRa Colab å¿«é€Ÿå‚è€ƒ

ä¸€é¡µé€ŸæŸ¥æŒ‡å— - æ‰“å°å‡ºæ¥éšæ—¶æŸ¥çœ‹

---

## âš¡ 5 åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

```bash
1. ä¸Šä¼  training_colab_complete.ipynb åˆ° Colab
2. è¿è¡Œæ—¶ â†’ GPU (T4/V100/A100) + High RAM
3. è¿è¡Œæ—¶ â†’ å…¨éƒ¨è¿è¡Œ
4. ç­‰å¾… 1-2 å°æ—¶
5. ä¸‹è½½æ¨¡å‹åˆ° Google Drive
```

---

## ğŸ“Š GPU é€‰æ‹©æŒ‡å—

| GPU | æ˜¾å­˜ | é€Ÿåº¦ | æ‰¹æ¬¡ | æ ·æœ¬æ•° | ä»·æ ¼ |
|-----|------|------|------|--------|------|
| T4 | 16GB | 1x | 32 | 200 | å…è´¹ |
| V100 | 32GB | 2x | 64 | 500 | Pro |
| A100 | 40GB | 4x | 128 | 1000+ | Pro+ |

**æ¨èï¼š** é¦–æ¬¡æµ‹è¯•ç”¨ T4ï¼Œæ­£å¼è®­ç»ƒç”¨ A100

---

## ğŸ¯ è®­ç»ƒé˜¶æ®µé€ŸæŸ¥

### Stage 1: å‹ç¼©é¢„è®­ç»ƒ
```python
æ•°æ®: pretrain_data.jsonl (QA æ ¼å¼)
æ—¶é—´: T4=30min, A100=10min
è¾“å‡º: /content/checkpoints/clara_stage1
```

### Stage 2: æŒ‡ä»¤å¾®è°ƒ
```python
æ•°æ®: instruction_data.jsonl (é—®ç­”æ ¼å¼)
æ—¶é—´: T4=30min, A100=10min
è¾“å‡º: /content/checkpoints/clara_stage2
```

### Stage 3: ç«¯åˆ°ç«¯
```python
æ•°æ®: end_to_end_data.jsonl (åŒ Stage 2)
æ—¶é—´: T4=45min, A100=15min
è¾“å‡º: /content/checkpoints/clara_stage3_final âœ…
```

---

## ğŸ“ æ•°æ®æ ¼å¼

### Stage 1 æ ¼å¼
```json
{
  "data_type": "qa",
  "question": ["é—®é¢˜"],
  "answers": ["ç­”æ¡ˆ"],
  "docs": ["æ–‡æ¡£"]
}
```

### Stage 2/3 æ ¼å¼
```json
{
  "question": "é—®é¢˜",
  "docs": ["æ–‡æ¡£1", "æ–‡æ¡£2"],
  "gold_answer": "ç­”æ¡ˆ"
}
```

---

## âŒ OOM é”™è¯¯ - å¿«é€Ÿä¿®å¤

```python
# ä¿®æ”¹é…ç½®å•å…ƒæ ¼
TRAIN_BATCH_SIZE = 16      # å‡å°
MICRO_BATCH_SIZE = 1       # ä¿æŒ
MAX_SAMPLES = 100          # å‡å°‘
MAX_LEN = 1024             # å‡åŠ
```

---

## âš™ï¸ å…³é”®å‚æ•°é€ŸæŸ¥

```python
# æ‰¹æ¬¡å¤§å°ï¼ˆè°ƒæ•´ä»¥é€‚åº” GPUï¼‰
TRAIN_BATCH_SIZE = 32      # T4
TRAIN_BATCH_SIZE = 128     # A100

# è®­ç»ƒæ ·æœ¬
MAX_SAMPLES = 200          # æµ‹è¯•
MAX_SAMPLES = 1000+        # ç”Ÿäº§

# å­¦ä¹ ç‡
LEARNING_RATE = 1e-4       # é»˜è®¤
LEARNING_RATE = 5e-5       # ä¿å®ˆ
LEARNING_RATE = 2e-4       # æ¿€è¿›

# å‹ç¼©ç‡
COMPRESS_RATE = 32         # æ¨è
COMPRESS_RATE = 64         # æ›´é«˜å‹ç¼©

# Flash Attention
USE_FLASH_ATTN = False     # è·³è¿‡ï¼ˆç¨³å®šï¼‰
USE_FLASH_ATTN = True      # åŠ é€Ÿ 15%
```

---

## ğŸš¨ å¸¸è§é”™è¯¯é€ŸæŸ¥

| é”™è¯¯ | åŸå›  | è§£å†³ |
|------|------|------|
| CUDA OOM | æ˜¾å­˜ä¸è¶³ | å‡å° BATCH_SIZE |
| Checkpoint not found | ä¸Šé˜¶æ®µå¤±è´¥ | æ£€æŸ¥ /content/checkpoints/ |
| JSON decode error | æ•°æ®æ ¼å¼é”™è¯¯ | éªŒè¯ .jsonl æ ¼å¼ |
| RuntimeError: no GPU | æœªå¯ç”¨ GPU | åˆ‡æ¢åˆ° GPU è¿è¡Œæ—¶ |
| Disconnected | è¶…æ—¶æ–­å¼€ | å®šæœŸç‚¹å‡»é¡µé¢ |

---

## ğŸ“¤ æ¨¡å‹å¯¼å‡º 3 æ­¥

### æ–¹æ³• 1: ä¸‹è½½
```python
!zip -r model.zip /content/checkpoints/clara_stage3_final/
from google.colab import files
files.download('/content/checkpoints/model.zip')
```

### æ–¹æ³• 2: Google Drive
```python
from google.colab import drive
drive.mount('/content/drive')
!cp -r /content/checkpoints/clara_stage3_final \
  /content/drive/MyDrive/
```

### æ–¹æ³• 3: HuggingFace
```python
!pip install huggingface_hub
from huggingface_hub import HfApi, login
login()
api = HfApi()
api.upload_folder(
    folder_path="/content/checkpoints/clara_stage3_final",
    repo_id="username/clara-model"
)
```

---

## ğŸ§ª å¿«é€Ÿæµ‹è¯•ä»£ç 

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_path = "/content/checkpoints/clara_stage3_final"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

def ask(question, doc):
    prompt = f"Document: {doc}\n\nQuestion: {question}\n\nAnswer:"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=100)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# æµ‹è¯•
answer = ask("What is CLaRa?", "CLaRa is a RAG framework...")
print(answer)
```

---

## â±ï¸ é¢„è®¡æ—¶é—´

### T4 (16GB) - å…è´¹
- Stage 1: 30-60 åˆ†é’Ÿ
- Stage 2: 30-60 åˆ†é’Ÿ
- Stage 3: 45-90 åˆ†é’Ÿ
- **æ€»è®¡: 2-3 å°æ—¶**

### A100 (40GB) - Pro
- Stage 1: 10-20 åˆ†é’Ÿ
- Stage 2: 10-20 åˆ†é’Ÿ
- Stage 3: 15-30 åˆ†é’Ÿ
- **æ€»è®¡: 40-70 åˆ†é’Ÿ**

---

## ğŸ’° æˆæœ¬ä¼°ç®—

| é…ç½® | GPU | æ—¶é—´ | æ•°æ® | æˆæœ¬ |
|------|-----|------|------|------|
| æµ‹è¯• | T4 | 2h | 200æ¡ | å…è´¹ |
| å°è§„æ¨¡ | V100 | 1.5h | 500æ¡ | $0.5 |
| ä¸­è§„æ¨¡ | A100 | 1h | 2Kæ¡ | $2 |
| å¤§è§„æ¨¡ | A100 | 3h | 10Kæ¡ | $6 |

---

## ğŸ“ è·å–å¸®åŠ©

1. **æ–‡æ¡£**: [COLAB_TRAINING_GUIDE.md](COLAB_TRAINING_GUIDE.md)
2. **GitHub**: [ml-clara Issues](https://github.com/apple/ml-clara/issues)
3. **Flash Attn**: [è®­ç»ƒæŒ‡å— Q5](COLAB_TRAINING_GUIDE.md#q5-flash-attention-å®‰è£…å¤±è´¥)

---

## âœ… æ£€æŸ¥æ¸…å•

**è®­ç»ƒå‰**
- [ ] GPU è¿è¡Œæ—¶å·²å¯ç”¨ï¼ˆT4/V100/A100ï¼‰
- [ ] High-RAM å·²é€‰æ‹©
- [ ] æ•°æ®æ–‡ä»¶å·²å‡†å¤‡ï¼ˆ.jsonl æ ¼å¼ï¼‰
- [ ] ç£ç›˜ç©ºé—´å……è¶³ï¼ˆ30GB+ï¼‰

**è®­ç»ƒå**
- [ ] 3 ä¸ªé˜¶æ®µéƒ½å®Œæˆ
- [ ] æ£€æŸ¥ç‚¹åœ¨ /content/checkpoints/
- [ ] æ¨¡å‹å¯ä»¥åŠ è½½
- [ ] æ¨ç†æµ‹è¯•é€šè¿‡
- [ ] æ¨¡å‹å·²å¤‡ä»½

---

## ğŸ“ ä¸“ä¸šæç¤º

1. **é¦–æ¬¡è¿è¡Œ**: ç”¨ç¤ºä¾‹æ•°æ® + T4 GPU æµ‹è¯•æµç¨‹
2. **æ•°æ®å‡†å¤‡**: åœ¨æœ¬åœ°ç”Ÿæˆï¼ŒéªŒè¯åä¸Šä¼ 
3. **æ­£å¼è®­ç»ƒ**: Colab Pro + A100 + å®Œæ•´æ•°æ®
4. **ä¿å­˜æ¨¡å‹**: ç«‹å³å¤‡ä»½åˆ° Google Drive
5. **ç›‘æ§è®­ç»ƒ**: æ¯ 30 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡è¿›åº¦

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| é…ç½® | æ¯ç§’æ ·æœ¬æ•° | æ¯å°æ—¶æ ·æœ¬æ•° |
|------|-----------|-------------|
| T4 + batch=32 | ~10 | ~36,000 |
| V100 + batch=64 | ~20 | ~72,000 |
| A100 + batch=128 | ~40 | ~144,000 |

---

## ğŸ”— ç›¸å…³é“¾æ¥

- ğŸ“„ [Paper](https://arxiv.org/abs/2511.18659)
- ğŸ’» [GitHub](https://github.com/apple/ml-clara)
- ğŸ¤— [Models](https://huggingface.co/probejie)
- ğŸ“š [å®Œæ•´æŒ‡å—](COLAB_TRAINING_GUIDE.md)

---

**ç‰ˆæœ¬**: 1.0 | **æ—¥æœŸ**: 2025-12-01
**æ‰“å°æ­¤é¡µä»¥ä¾›å¿«é€Ÿå‚è€ƒ**
