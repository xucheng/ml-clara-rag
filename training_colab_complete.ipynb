{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udd16 CLaRa Training on Colab\n",
        "\n",
        "[![Paper](https://img.shields.io/badge/Paper-Arxiv-green)](https://arxiv.org/abs/2511.18659)\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-ml--clara-blue)](https://github.com/apple/ml-clara)\n",
        "\n",
        "**CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning**\n",
        "\n",
        "This notebook provides a complete training pipeline for CLaRa model on Google Colab.\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udccb Training Pipeline\n",
        "\n",
        "1. **Environment Setup** - Check GPU, install dependencies\n",
        "2. **Code & Data Preparation** - Clone repository, prepare training data\n",
        "3. **Stage 1: Compression Pretraining** - Train compressor with KPCP framework\n",
        "4. **Stage 2: Instruction Tuning** - Fine-tune on instruction-following tasks\n",
        "5. **Stage 3: End-to-End Training** - Joint training of reranker and generator\n",
        "6. **Model Inference & Export** - Test model and save checkpoints\n",
        "\n",
        "---\n",
        "\n",
        "### \u2699\ufe0f Configuration\n",
        "\n",
        "**Hardware Requirements:**\n",
        "- GPU: T4 (16GB) or better (A100 recommended)\n",
        "- RAM: High RAM (25GB+)\n",
        "- Runtime: GPU with High RAM\n",
        "\n",
        "**Training Settings:**\n",
        "- Base Model: `mistralai/Mistral-7B-Instruct-v0.2`\n",
        "- Compression Rate: 32x\n",
        "- Training Framework: OpenRLHF + DeepSpeed ZeRO-2\n",
        "- Batch Size: Adaptive based on available GPU memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1\ufe0f\u20e3 Environment Setup\n",
        "\n",
        "Check GPU availability and system information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check GPU and CUDA\n",
        "!nvidia-smi\n",
        "print('\\n' + '='*60)\n",
        "import torch\n",
        "print(f'PyTorch Version: {torch.__version__}')\n",
        "print(f'CUDA Available: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'CUDA Version: {torch.version.cuda}')\n",
        "    print(f'GPU Device: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')\n",
        "print('='*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2\ufe0f\u20e3 Install Dependencies\n",
        "\n",
        "Install required packages. This may take 5-10 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%time\n",
        "# Install core dependencies\n",
        "!pip install -q accelerate==1.10.1 transformers==4.56.2 datasets==3.2.0 \\\n",
        "    peft==0.17.1 deepspeed==0.18.1 wandb==0.22.2 \\\n",
        "    einops==0.8.1 sentencepiece==0.2.0 tiktoken==0.11.0\n",
        "\n",
        "print('\u2705 Core dependencies installed')\n",
        "\n",
        "# Check DeepSpeed\n",
        "import deepspeed\n",
        "print(f'DeepSpeed version: {deepspeed.__version__}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Install Flash Attention (Recommended for speed)\n",
        "\n",
        "Flash Attention can speed up training by ~15%. Skip if installation fails."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Option 1: Try precompiled version (fast)\n",
        "# !pip install flash-attn --no-build-isolation\n",
        "\n",
        "# Option 2: Skip flash-attn (training still works)\n",
        "print('\u26a0\ufe0f Skipping flash-attn installation')\n",
        "print('Training will use standard attention (slightly slower but fully functional)')\n",
        "USE_FLASH_ATTN = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3\ufe0f\u20e3 Download Code and Data\n",
        "\n",
        "Clone CLaRa repository and OpenRLHF framework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%time\n",
        "import os\n",
        "\n",
        "# Clone CLaRa repository\n",
        "if not os.path.exists('ml-clara'):\n",
        "    !git clone https://github.com/apple/ml-clara.git\n",
        "    print('\u2705 CLaRa repository cloned')\n",
        "else:\n",
        "    print('\u2705 CLaRa repository already exists')\n",
        "\n",
        "# Clone OpenRLHF\n",
        "if not os.path.exists('OpenRLHF_repo'):\n",
        "    !git clone https://github.com/OpenRLHF/OpenRLHF.git OpenRLHF_repo\n",
        "    !cp -R -n OpenRLHF_repo/openrlhf/* ml-clara/openrlhf/\n",
        "    print('\u2705 OpenRLHF framework integrated')\n",
        "else:\n",
        "    print('\u2705 OpenRLHF already integrated')\n",
        "\n",
        "# Change to project directory\n",
        "%cd ml-clara\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4\ufe0f\u20e3 Data Preparation\n",
        "\n",
        "You have two options:\n",
        "1. **Use example data** (provided in repository) - Quick start\n",
        "2. **Upload your own data** - For custom training\n",
        "\n",
        "### Data Format\n",
        "\n",
        "**Stage 1 (Pretraining)**: `pretrain_data.jsonl`\n",
        "```json\n",
        "{\"data_type\": \"qa\", \"question\": [\"Q1\"], \"answers\": [\"A1\"], \"docs\": [\"doc1\"]}\n",
        "```\n",
        "\n",
        "**Stage 2 (Instruction Tuning)**: `instruction_data.jsonl`\n",
        "```json\n",
        "{\"question\": \"Q1\", \"docs\": [\"doc1\"], \"gold_answer\": \"A1\"}\n",
        "```\n",
        "\n",
        "**Stage 3 (End-to-End)**: `end_to_end_data.jsonl` (same as Stage 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check example data\n",
        "!ls -lh example/*.jsonl\n",
        "print('\\n\ud83d\udcca Example data statistics:')\n",
        "!wc -l example/*.jsonl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option A: Use Example Data (Recommended for first run)\n",
        "\n",
        "The repository includes small example datasets for quick testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Use example data (already in repository)\n",
        "DATA_MODE = 'example'  # or 'custom'\n",
        "\n",
        "if DATA_MODE == 'example':\n",
        "    PRETRAIN_DATA = 'example/pretrain_data.jsonl'\n",
        "    INSTRUCTION_DATA = 'example/instruction_data.jsonl'\n",
        "    END_TO_END_DATA = 'example/end_to_end_data.jsonl'\n",
        "    print('\u2705 Using example data from repository')\n",
        "    print(f'  - Pretraining: {PRETRAIN_DATA}')\n",
        "    print(f'  - Instruction: {INSTRUCTION_DATA}')\n",
        "    print(f'  - End-to-End: {END_TO_END_DATA}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option B: Upload Your Own Data\n",
        "\n",
        "Upload your custom training data files. Uncomment and run if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Uncomment to upload custom data\n",
        "# from google.colab import files\n",
        "# \n",
        "# print('Upload pretrain_data.jsonl:')\n",
        "# uploaded = files.upload()\n",
        "# !mkdir -p data\n",
        "# !mv pretrain_data.jsonl data/\n",
        "# \n",
        "# print('Upload instruction_data.jsonl:')\n",
        "# uploaded = files.upload()\n",
        "# !mv instruction_data.jsonl data/\n",
        "# \n",
        "# print('Upload end_to_end_data.jsonl:')\n",
        "# uploaded = files.upload()\n",
        "# !mv end_to_end_data.jsonl data/\n",
        "# \n",
        "# DATA_MODE = 'custom'\n",
        "# PRETRAIN_DATA = 'data/pretrain_data.jsonl'\n",
        "# INSTRUCTION_DATA = 'data/instruction_data.jsonl'\n",
        "# END_TO_END_DATA = 'data/end_to_end_data.jsonl'\n",
        "# print('\u2705 Custom data uploaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5\ufe0f\u20e3 Training Configuration\n",
        "\n",
        "Set up training parameters. Adjust based on your GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "\n",
        "# Detect GPU memory and set batch sizes\n",
        "if torch.cuda.is_available():\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f'GPU Memory: {gpu_memory:.1f} GB')\n",
        "    \n",
        "    if gpu_memory < 20:  # T4 (16GB)\n",
        "        TRAIN_BATCH_SIZE = 32\n",
        "        MICRO_BATCH_SIZE = 1\n",
        "        NUM_GPUS = 1\n",
        "        MAX_SAMPLES = 200\n",
        "        print('\u2699\ufe0f Using T4 config (16GB)')\n",
        "    elif gpu_memory < 50:  # V100 or A100-40GB\n",
        "        TRAIN_BATCH_SIZE = 64\n",
        "        MICRO_BATCH_SIZE = 2\n",
        "        NUM_GPUS = 1\n",
        "        MAX_SAMPLES = 500\n",
        "        print('\u2699\ufe0f Using V100/A100-40GB config')\n",
        "    else:  # A100-80GB\n",
        "        TRAIN_BATCH_SIZE = 128\n",
        "        MICRO_BATCH_SIZE = 2\n",
        "        NUM_GPUS = 1\n",
        "        MAX_SAMPLES = 1000\n",
        "        print('\u2699\ufe0f Using A100-80GB config')\n",
        "else:\n",
        "    raise RuntimeError('\u274c No GPU available. Please enable GPU runtime.')\n",
        "\n",
        "# Model and checkpoint paths\n",
        "MODEL_PATH = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
        "CHECKPOINT_DIR = '/content/checkpoints'\n",
        "\n",
        "# Training settings\n",
        "LEARNING_RATE = 1e-4\n",
        "MAX_EPOCHS = 1\n",
        "COMPRESS_RATE = 32\n",
        "DOC_MAX_LENGTH = 256\n",
        "MAX_LEN = 2048\n",
        "\n",
        "# Flash attention flag\n",
        "FLASH_ATTN_FLAG = '--flash_attn' if USE_FLASH_ATTN else ''\n",
        "\n",
        "print(f'\\n\ud83d\udcdd Training Configuration:')\n",
        "print(f'  Model: {MODEL_PATH}')\n",
        "print(f'  Batch Size: {TRAIN_BATCH_SIZE}')\n",
        "print(f'  Micro Batch Size: {MICRO_BATCH_SIZE}')\n",
        "print(f'  Max Samples: {MAX_SAMPLES}')\n",
        "print(f'  Learning Rate: {LEARNING_RATE}')\n",
        "print(f'  Compress Rate: {COMPRESS_RATE}x')\n",
        "print(f'  Flash Attention: {USE_FLASH_ATTN}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6\ufe0f\u20e3 Stage 1: Compression Pretraining\n",
        "\n",
        "Train the compressor using KPCP framework with QA pairs and paraphrases.\n",
        "\n",
        "**What happens:**\n",
        "- Compress documents into continuous latent representations\n",
        "- Learn semantic compression through QA-based supervision\n",
        "- Support compression rates from 1x to 256x\n",
        "\n",
        "**Expected time:** 10-30 minutes (depends on data size and GPU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%time\n",
        "# Stage 1: Compression Pretraining\n",
        "print('\ud83d\ude80 Starting Stage 1: Compression Pretraining\\n')\n",
        "\n",
        "!torchrun --nproc_per_node={NUM_GPUS} \\\n",
        "    --master_port=29500 \\\n",
        "    -m openrlhf.cli.train_sft \\\n",
        "    --max_len {MAX_LEN} \\\n",
        "    --dataset {PRETRAIN_DATA} \\\n",
        "    --pretrain {MODEL_PATH} \\\n",
        "    --train_batch_size {TRAIN_BATCH_SIZE} \\\n",
        "    --micro_train_batch_size {MICRO_BATCH_SIZE} \\\n",
        "    --max_samples {MAX_SAMPLES} \\\n",
        "    --save_path {CHECKPOINT_DIR}/clara_stage1 \\\n",
        "    --save_steps -2 \\\n",
        "    --logging_steps 5 \\\n",
        "    --eval_steps -1 \\\n",
        "    --zero_stage 2 \\\n",
        "    --max_epochs {MAX_EPOCHS} \\\n",
        "    --bf16 \\\n",
        "    {FLASH_ATTN_FLAG} \\\n",
        "    --learning_rate {LEARNING_RATE} \\\n",
        "    --stage stage1 \\\n",
        "    --generation_top_k 1 \\\n",
        "    --qa_loss \\\n",
        "    --doc_max_length {DOC_MAX_LENGTH} \\\n",
        "    --compress_rate {COMPRESS_RATE} \\\n",
        "    --mse_loss \\\n",
        "    --gradient_checkpointing\n",
        "\n",
        "print('\\n\u2705 Stage 1 completed!')\n",
        "print(f'Checkpoint saved to: {CHECKPOINT_DIR}/clara_stage1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check Stage 1 Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verify checkpoint\n",
        "!ls -lh {CHECKPOINT_DIR}/clara_stage1/\n",
        "!du -sh {CHECKPOINT_DIR}/clara_stage1/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7\ufe0f\u20e3 Stage 2: Instruction Tuning\n",
        "\n",
        "Fine-tune the compressor on instruction-following tasks.\n",
        "\n",
        "**What happens:**\n",
        "- Load Stage 1 checkpoint\n",
        "- Fine-tune on downstream QA tasks\n",
        "- Ensure compressed representations retain sufficient semantics\n",
        "\n",
        "**Expected time:** 10-30 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%time\n",
        "# Stage 2: Instruction Tuning\n",
        "print('\ud83d\ude80 Starting Stage 2: Instruction Tuning\\n')\n",
        "\n",
        "!torchrun --nproc_per_node={NUM_GPUS} \\\n",
        "    --master_port=29500 \\\n",
        "    -m openrlhf.cli.train_sft \\\n",
        "    --max_len {MAX_LEN} \\\n",
        "    --dataset {INSTRUCTION_DATA} \\\n",
        "    --pretrain {MODEL_PATH} \\\n",
        "    --ckpt_path {CHECKPOINT_DIR}/clara_stage1 \\\n",
        "    --train_batch_size {TRAIN_BATCH_SIZE} \\\n",
        "    --micro_train_batch_size {MICRO_BATCH_SIZE} \\\n",
        "    --max_samples {MAX_SAMPLES} \\\n",
        "    --save_path {CHECKPOINT_DIR}/clara_stage2 \\\n",
        "    --save_steps -2 \\\n",
        "    --logging_steps 5 \\\n",
        "    --eval_steps -1 \\\n",
        "    --zero_stage 2 \\\n",
        "    --max_epochs {MAX_EPOCHS} \\\n",
        "    --bf16 \\\n",
        "    {FLASH_ATTN_FLAG} \\\n",
        "    --learning_rate {LEARNING_RATE} \\\n",
        "    --stage stage2 \\\n",
        "    --generation_top_k 1 \\\n",
        "    --doc_max_length {DOC_MAX_LENGTH} \\\n",
        "    --compress_rate {COMPRESS_RATE} \\\n",
        "    --gradient_checkpointing\n",
        "\n",
        "print('\\n\u2705 Stage 2 completed!')\n",
        "print(f'Checkpoint saved to: {CHECKPOINT_DIR}/clara_stage2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verify checkpoint\n",
        "!ls -lh {CHECKPOINT_DIR}/clara_stage2/\n",
        "!du -sh {CHECKPOINT_DIR}/clara_stage2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8\ufe0f\u20e3 Stage 3: End-to-End Fine-tuning\n",
        "\n",
        "Jointly train reranker and generator.\n",
        "\n",
        "**What happens:**\n",
        "- Load Stage 2 checkpoint\n",
        "- Unify retrieval and generation in shared continuous space\n",
        "- Use differentiable top-k estimator\n",
        "- Train via single language modeling loss\n",
        "\n",
        "**Expected time:** 15-40 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%time\n",
        "# Stage 3: End-to-End Training\n",
        "print('\ud83d\ude80 Starting Stage 3: End-to-End Fine-tuning\\n')\n",
        "\n",
        "!torchrun --nproc_per_node={NUM_GPUS} \\\n",
        "    --master_port=29500 \\\n",
        "    -m openrlhf.cli.train_sft \\\n",
        "    --max_len {MAX_LEN} \\\n",
        "    --dataset {END_TO_END_DATA} \\\n",
        "    --pretrain {MODEL_PATH} \\\n",
        "    --ckpt_path {CHECKPOINT_DIR}/clara_stage2 \\\n",
        "    --train_batch_size {TRAIN_BATCH_SIZE} \\\n",
        "    --micro_train_batch_size {MICRO_BATCH_SIZE} \\\n",
        "    --max_samples {MAX_SAMPLES} \\\n",
        "    --save_path {CHECKPOINT_DIR}/clara_stage3_final \\\n",
        "    --save_steps -2 \\\n",
        "    --logging_steps 5 \\\n",
        "    --eval_steps -1 \\\n",
        "    --zero_stage 2 \\\n",
        "    --max_epochs {MAX_EPOCHS} \\\n",
        "    --bf16 \\\n",
        "    {FLASH_ATTN_FLAG} \\\n",
        "    --learning_rate {LEARNING_RATE} \\\n",
        "    --stage stage3 \\\n",
        "    --generation_top_k 5 \\\n",
        "    --doc_max_length {DOC_MAX_LENGTH} \\\n",
        "    --compress_rate {COMPRESS_RATE} \\\n",
        "    --gradient_checkpointing\n",
        "\n",
        "print('\\n\u2705 Stage 3 completed!')\n",
        "print(f'Final model saved to: {CHECKPOINT_DIR}/clara_stage3_final')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verify final checkpoint\n",
        "!ls -lh {CHECKPOINT_DIR}/clara_stage3_final/\n",
        "!du -sh {CHECKPOINT_DIR}/clara_stage3_final/\n",
        "\n",
        "print('\\n\ud83c\udf89 Training pipeline completed!')\n",
        "print('\\n\ud83d\udcc1 All checkpoints:')\n",
        "!ls -lh {CHECKPOINT_DIR}/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 9\ufe0f\u20e3 Model Inference Test\n",
        "\n",
        "Test the trained model with a sample query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load trained model for inference\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load model\n",
        "model_path = f'{CHECKPOINT_DIR}/clara_stage3_final'\n",
        "print(f'Loading model from: {model_path}')\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map='auto'\n",
        "    )\n",
        "    print('\u2705 Model loaded successfully')\n",
        "    \n",
        "    # Test inference\n",
        "    test_query = \"What is CLaRa?\"\n",
        "    test_doc = \"CLaRa is a framework that bridges retrieval and generation with continuous latent reasoning.\"\n",
        "    \n",
        "    prompt = f\"Document: {test_doc}\\n\\nQuestion: {test_query}\\n\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f'\\n\ud83d\udcdd Test Query: {test_query}')\n",
        "    print(f'\ud83e\udd16 Model Response:')\n",
        "    print(response)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f'\u274c Error loading model: {e}')\n",
        "    print('This is expected if training was skipped or checkpoint format needs adjustment.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## \ud83d\udd1f Export and Save Model\n",
        "\n",
        "Download your trained model to local machine or save to Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option A: Download to Local Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a zip archive of the final model\n",
        "!apt-get install -y zip\n",
        "!cd {CHECKPOINT_DIR} && zip -r clara_stage3_final.zip clara_stage3_final/\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "# files.download(f'{CHECKPOINT_DIR}/clara_stage3_final.zip')\n",
        "print(f'Model archived to: {CHECKPOINT_DIR}/clara_stage3_final.zip')\n",
        "print('Uncomment the download line above to download to your local machine')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option B: Save to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Copy checkpoint to Drive\n",
        "# !cp -r {CHECKPOINT_DIR}/clara_stage3_final /content/drive/MyDrive/\n",
        "# print('\u2705 Model saved to Google Drive')\n",
        "\n",
        "print('Uncomment the lines above to save to Google Drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## \u2705 Training Summary\n",
        "\n",
        "### Checkpoints Created:\n",
        "1. **Stage 1**: `/content/checkpoints/clara_stage1` - Compression pretraining\n",
        "2. **Stage 2**: `/content/checkpoints/clara_stage2` - Instruction tuning\n",
        "3. **Stage 3**: `/content/checkpoints/clara_stage3_final` - Final end-to-end model\n",
        "\n",
        "### Next Steps:\n",
        "1. **Evaluation**: Use the evaluation scripts in `scripts/evaluation_*.sh`\n",
        "2. **Fine-tuning**: Continue training with your own data\n",
        "3. **Deployment**: Export model for inference\n",
        "\n",
        "### Useful Resources:\n",
        "- \ud83d\udcc4 [Paper](https://arxiv.org/abs/2511.18659)\n",
        "- \ud83d\udcbb [GitHub](https://github.com/apple/ml-clara)\n",
        "- \ud83e\udd17 [HuggingFace Models](https://huggingface.co/probejie)\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udcca Troubleshooting\n",
        "\n",
        "**Out of Memory (OOM):**\n",
        "- Reduce `TRAIN_BATCH_SIZE` and `MICRO_BATCH_SIZE`\n",
        "- Decrease `MAX_SAMPLES`\n",
        "- Use gradient checkpointing (already enabled)\n",
        "\n",
        "**Training too slow:**\n",
        "- Install flash-attn (see cell above)\n",
        "- Use A100 GPU instead of T4\n",
        "- Reduce data size for testing\n",
        "\n",
        "**Checkpoint loading errors:**\n",
        "- Verify checkpoint path exists\n",
        "- Check disk space\n",
        "- Ensure previous stage completed successfully\n",
        "\n",
        "---\n",
        "\n",
        "**Made with \u2764\ufe0f by the CLaRa Team**\n",
        "\n",
        "If you use this code, please cite:\n",
        "```bibtex\n",
        "@article{zhao2024clara,\n",
        "  title={CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning},\n",
        "  author={Zhao, Zhihao and others},\n",
        "  journal={arXiv preprint arXiv:2511.18659},\n",
        "  year={2024}\n",
        "}\n",
        "```\n"
      ]
    }
  ]
}