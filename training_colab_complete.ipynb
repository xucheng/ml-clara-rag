{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ CLaRa Training on Colab\n",
    "\n",
    "[![Paper](https://img.shields.io/badge/Paper-Arxiv-green)](https://arxiv.org/abs/2511.18659)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-ml--clara-blue)](https://github.com/apple/ml-clara)\n",
    "\n",
    "**CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning**\n",
    "\n",
    "This notebook provides a complete training pipeline for CLaRa model on Google Colab.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Training Pipeline\n",
    "\n",
    "1. **Environment Setup** - Check GPU, install dependencies\n",
    "2. **Code & Data Preparation** - Clone repository, prepare training data\n",
    "3. **Stage 1: Compression Pretraining** - Train compressor with KPCP framework\n",
    "4. **Stage 2: Instruction Tuning** - Fine-tune on instruction-following tasks\n",
    "5. **Stage 3: End-to-End Training** - Joint training of reranker and generator\n",
    "6. **Model Inference & Export** - Test model and save checkpoints\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Configuration\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- GPU: T4 (16GB) or better (A100 recommended)\n",
    "- RAM: High RAM (25GB+)\n",
    "- Runtime: GPU with High RAM\n",
    "\n",
    "**Training Settings:**\n",
    "- Base Model: `mistralai/Mistral-7B-Instruct-v0.2`\n",
    "- Compression Rate: 32x\n",
    "- Training Framework: OpenRLHF + DeepSpeed ZeRO-2\n",
    "- Batch Size: Adaptive based on available GPU memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1Ô∏è‚É£ Environment Setup\n",
    "\n",
    "Check GPU availability and system information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and CUDA\n",
    "!nvidia-smi\n",
    "print('\\n' + '='*60)\n",
    "import torch\n",
    "print(f'PyTorch Version: {torch.__version__}')\n",
    "print(f'CUDA Available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA Version: {torch.version.cuda}')\n",
    "    print(f'GPU Device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2Ô∏è‚É£ Install Dependencies\n",
    "\n",
    "Install required packages. This may take 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\n# Install core dependencies\nprint('üì¶ Installing core dependencies...')\n\n# Install basic packages first\n!pip install -q accelerate==1.10.1 transformers==4.56.2 datasets==3.2.0 \\\n    peft==0.17.1 einops==0.8.1 sentencepiece==0.2.0 tiktoken==0.11.0\n\nprint('‚úÖ Core packages installed')\n\n# Fix fsspec version conflict with gcsfs\nprint('\\nüì¶ Fixing fsspec version conflict...')\n!pip install -q --upgrade fsspec==2025.3.0\nprint('‚úÖ fsspec upgraded to match gcsfs requirements')\n\n# Install DeepSpeed (may fail on some systems)\nprint('\\nüì¶ Installing DeepSpeed...')\ntry:\n    !pip install -q deepspeed==0.18.1\n    import deepspeed\n    print(f'‚úÖ DeepSpeed {deepspeed.__version__} installed')\nexcept Exception as e:\n    print(f'‚ö†Ô∏è  DeepSpeed installation failed: {e}')\n    print('   Training will continue without DeepSpeed optimizations')\n\n# Install WandB (optional)\nprint('\\nüì¶ Installing WandB (optional)...')\n!pip install -q wandb==0.22.2\nprint('‚úÖ WandB installed')\n\nprint('\\nüéâ Dependencies installation complete!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Install Flash Attention (Recommended for speed)\n",
    "\n",
    "Flash Attention can speed up training by ~15%. Skip if installation fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Try precompiled version (fast)\n",
    "# !pip install flash-attn --no-build-isolation\n",
    "\n",
    "# Option 2: Skip flash-attn (training still works)\n",
    "print('‚ö†Ô∏è Skipping flash-attn installation')\n",
    "print('Training will use standard attention (slightly slower but fully functional)')\n",
    "USE_FLASH_ATTN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3Ô∏è‚É£ Download Code and Data\n\nClone CLaRa repository and OpenRLHF framework.\n\n**Note:** This notebook uses a custom fork with the following fixes:\n- ‚úÖ Flash Attention made optional (with fallback implementations)\n- ‚úÖ Dependency conflicts resolved (fsspec, gcsfs)\n- ‚úÖ Colab-optimized training pipeline\n\nThe custom fork ensures training works smoothly without requiring flash_attn installation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\nimport os\n\n# Clone CLaRa repository (with flash_attn fallback fixes and complete OpenRLHF integration)\nif not os.path.exists('ml-clara'):\n    # Use custom fork with fixes instead of official repo\n    !git clone https://github.com/xucheng/ml-clara-rag.git ml-clara\n    print('‚úÖ CLaRa repository cloned (with fixes)')\n    print('   - Includes flash_attn fallback implementation')\n    print('   - Includes complete OpenRLHF framework')\nelse:\n    print('‚úÖ CLaRa repository already exists')\n    # Pull latest changes if repository exists\n    print('Pulling latest changes...')\n    !cd ml-clara && git pull origin main\n    print('‚úÖ Repository updated')\n\n# Verify OpenRLHF is included\nprint('\\nüì¶ Verifying OpenRLHF framework...')\nif os.path.exists('ml-clara/openrlhf'):\n    import subprocess\n    file_count = subprocess.check_output(\n        'find ml-clara/openrlhf -type f -name \"*.py\" | wc -l',\n        shell=True\n    ).decode().strip()\n    print(f'‚úÖ OpenRLHF framework ready ({file_count} Python files)')\nelse:\n    print('‚ùå OpenRLHF not found - please check repository')\n\n# Change to project directory\n%cd ml-clara\n!pwd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4Ô∏è‚É£ Data Preparation\n",
    "\n",
    "You have two options:\n",
    "1. **Use example data** (provided in repository) - Quick start\n",
    "2. **Upload your own data** - For custom training\n",
    "\n",
    "### Data Format\n",
    "\n",
    "**Stage 1 (Pretraining)**: `pretrain_data.jsonl`\n",
    "```json\n",
    "{\"data_type\": \"qa\", \"question\": [\"Q1\"], \"answers\": [\"A1\"], \"docs\": [\"doc1\"]}\n",
    "```\n",
    "\n",
    "**Stage 2 (Instruction Tuning)**: `instruction_data.jsonl`\n",
    "```json\n",
    "{\"question\": \"Q1\", \"docs\": [\"doc1\"], \"gold_answer\": \"A1\"}\n",
    "```\n",
    "\n",
    "**Stage 3 (End-to-End)**: `end_to_end_data.jsonl` (same as Stage 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check example data\n",
    "!ls -lh example/*.jsonl\n",
    "print('\\nüìä Example data statistics:')\n",
    "!wc -l example/*.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Use Example Data (Recommended for first run)\n",
    "\n",
    "The repository includes small example datasets for quick testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use example data (already in repository)\n",
    "DATA_MODE = 'example'  # or 'custom'\n",
    "\n",
    "if DATA_MODE == 'example':\n",
    "    PRETRAIN_DATA = 'example/pretrain_data.jsonl'\n",
    "    INSTRUCTION_DATA = 'example/instruction_data.jsonl'\n",
    "    END_TO_END_DATA = 'example/end_to_end_data.jsonl'\n",
    "    print('‚úÖ Using example data from repository')\n",
    "    print(f'  - Pretraining: {PRETRAIN_DATA}')\n",
    "    print(f'  - Instruction: {INSTRUCTION_DATA}')\n",
    "    print(f'  - End-to-End: {END_TO_END_DATA}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Option B: Load from Google Drive\n\nMount Google Drive and use data files stored there.\n\n**Example paths in Google Drive:**\n- `/content/drive/MyDrive/Colab Notebooks/data/ml-clara/pretrain_data.jsonl`\n- `/content/drive/MyDrive/data/ml-clara/instruction_data.jsonl`\n\nRun the cell below to mount Drive and set paths."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Detect environment\ntry:\n    from google.colab import drive\n    IS_COLAB = True\nexcept ImportError:\n    IS_COLAB = False\n\nif IS_COLAB:\n    # Mount Google Drive\n    print('üìÇ Mounting Google Drive...')\n    drive.mount('/content/drive')\n    print('‚úÖ Google Drive mounted at /content/drive')\n    \n    # Modify these paths to match your Drive folder structure\n    # Example: If your files are in \"Colab Notebooks/data/ml-clara/\"\n    DRIVE_BASE = '/content/drive/MyDrive/Colab Notebooks/data/ml-clara'\n    \n    PRETRAIN_DATA = f'{DRIVE_BASE}/pretrain_data.jsonl'\n    INSTRUCTION_DATA = f'{DRIVE_BASE}/instruction_data.jsonl'\n    END_TO_END_DATA = f'{DRIVE_BASE}/end_to_end_data.jsonl'\n    \n    print(f'\\nüìÅ Looking for data in: {DRIVE_BASE}')\n    \n    # Verify files exist\n    all_found = True\n    for name, path in [('Pretrain', PRETRAIN_DATA),\n                       ('Instruction', INSTRUCTION_DATA),\n                       ('End-to-End', END_TO_END_DATA)]:\n        if os.path.exists(path):\n            file_size = os.path.getsize(path) / 1024  # KB\n            print(f'‚úÖ {name}: {path} ({file_size:.1f} KB)')\n        else:\n            print(f'‚ùå {name}: {path} (NOT FOUND)')\n            all_found = False\n    \n    if all_found:\n        DATA_MODE = 'drive'\n        print(f'\\n‚úÖ All data files found in Google Drive')\n    else:\n        print(f'\\n‚ö†Ô∏è  Some files not found. Please check:')\n        print(f'   1. Files are uploaded to: {DRIVE_BASE}')\n        print(f'   2. Folder path is correct (including spaces)')\n        print(f'   3. File names match exactly')\n        print(f'\\nüí° To fix: Update DRIVE_BASE path in this cell')\nelse:\n    print('‚ö†Ô∏è  Not in Google Colab environment')\n    print('This cell is designed for Google Colab with Drive mounting')\n    print('Use Option A (example data) or Option C (local paths) instead')"
  },
  {
   "cell_type": "markdown",
   "source": "### Option C: Upload Files or Use Local Paths\n\nThis cell automatically detects your environment:\n\n**In Google Colab:**\n- Uses the upload widget (`files.upload()`)\n- Simply run the cell and select files when prompted\n\n**In Local/VS Code:**\n- Uses file paths instead\n- Modify the paths to point to your local data files\n\nRun the cell to load custom data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport sys\n\n# Detect environment\ntry:\n    from google.colab import files\n    IS_COLAB = True\nexcept ImportError:\n    IS_COLAB = False\n\nprint(f'Environment: {\"Google Colab\" if IS_COLAB else \"Local/VS Code\"}')\n\n# Option 1: For Google Colab - Use upload widget\nif IS_COLAB:\n    print('\\nüì§ Upload your custom data files:')\n    \n    print('\\n1Ô∏è‚É£ Upload pretrain_data.jsonl:')\n    uploaded = files.upload()\n    \n    print('\\n2Ô∏è‚É£ Upload instruction_data.jsonl:')\n    uploaded = files.upload()\n    \n    print('\\n3Ô∏è‚É£ Upload end_to_end_data.jsonl:')\n    uploaded = files.upload()\n    \n    # Move to data directory\n    !mkdir -p data\n    !mv pretrain_data.jsonl instruction_data.jsonl end_to_end_data.jsonl data/\n    \n    DATA_MODE = 'custom'\n    PRETRAIN_DATA = 'data/pretrain_data.jsonl'\n    INSTRUCTION_DATA = 'data/instruction_data.jsonl'\n    END_TO_END_DATA = 'data/end_to_end_data.jsonl'\n    print('\\n‚úÖ Custom data uploaded')\n\n# Option 2: For Local/VS Code - Specify file paths\nelse:\n    print('\\nüìÅ Using local file paths')\n    print('Please modify the paths below to point to your data files:\\n')\n    \n    # Modify these paths to your actual data locations\n    PRETRAIN_DATA = 'example/pretrain_data.jsonl'  # Change this\n    INSTRUCTION_DATA = 'example/instruction_data.jsonl'  # Change this\n    END_TO_END_DATA = 'example/end_to_end_data.jsonl'  # Change this\n    \n    # Verify files exist\n    missing_files = []\n    for name, path in [('Pretrain', PRETRAIN_DATA), \n                       ('Instruction', INSTRUCTION_DATA), \n                       ('End-to-End', END_TO_END_DATA)]:\n        if os.path.exists(path):\n            print(f'‚úÖ {name}: {path}')\n        else:\n            print(f'‚ùå {name}: {path} (NOT FOUND)')\n            missing_files.append(path)\n    \n    if missing_files:\n        print(f'\\n‚ö†Ô∏è  Warning: {len(missing_files)} file(s) not found')\n        print('Please update the file paths in this cell or use example data')\n    else:\n        DATA_MODE = 'custom'\n        print('\\n‚úÖ All custom data files found')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5Ô∏è‚É£ Training Configuration\n",
    "\n",
    "Set up training parameters. Adjust based on your GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Detect GPU memory and set batch sizes\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f'GPU Memory: {gpu_memory:.1f} GB')\n",
    "    \n",
    "    if gpu_memory < 20:  # T4 (16GB)\n",
    "        TRAIN_BATCH_SIZE = 32\n",
    "        MICRO_BATCH_SIZE = 1\n",
    "        NUM_GPUS = 1\n",
    "        MAX_SAMPLES = 200\n",
    "        print('‚öôÔ∏è Using T4 config (16GB)')\n",
    "    elif gpu_memory < 50:  # V100 or A100-40GB\n",
    "        TRAIN_BATCH_SIZE = 64\n",
    "        MICRO_BATCH_SIZE = 2\n",
    "        NUM_GPUS = 1\n",
    "        MAX_SAMPLES = 500\n",
    "        print('‚öôÔ∏è Using V100/A100-40GB config')\n",
    "    else:  # A100-80GB\n",
    "        TRAIN_BATCH_SIZE = 128\n",
    "        MICRO_BATCH_SIZE = 2\n",
    "        NUM_GPUS = 1\n",
    "        MAX_SAMPLES = 1000\n",
    "        print('‚öôÔ∏è Using A100-80GB config')\n",
    "else:\n",
    "    raise RuntimeError('‚ùå No GPU available. Please enable GPU runtime.')\n",
    "\n",
    "# Model and checkpoint paths\n",
    "MODEL_PATH = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "CHECKPOINT_DIR = '/content/checkpoints'\n",
    "\n",
    "# Training settings\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_EPOCHS = 1\n",
    "COMPRESS_RATE = 32\n",
    "DOC_MAX_LENGTH = 256\n",
    "MAX_LEN = 2048\n",
    "\n",
    "# Flash attention flag\n",
    "FLASH_ATTN_FLAG = '--flash_attn' if USE_FLASH_ATTN else ''\n",
    "\n",
    "print(f'\\nüìù Training Configuration:')\n",
    "print(f'  Model: {MODEL_PATH}')\n",
    "print(f'  Batch Size: {TRAIN_BATCH_SIZE}')\n",
    "print(f'  Micro Batch Size: {MICRO_BATCH_SIZE}')\n",
    "print(f'  Max Samples: {MAX_SAMPLES}')\n",
    "print(f'  Learning Rate: {LEARNING_RATE}')\n",
    "print(f'  Compress Rate: {COMPRESS_RATE}x')\n",
    "print(f'  Flash Attention: {USE_FLASH_ATTN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6Ô∏è‚É£ Stage 1: Compression Pretraining\n",
    "\n",
    "Train the compressor using KPCP framework with QA pairs and paraphrases.\n",
    "\n",
    "**What happens:**\n",
    "- Compress documents into continuous latent representations\n",
    "- Learn semantic compression through QA-based supervision\n",
    "- Support compression rates from 1x to 256x\n",
    "\n",
    "**Expected time:** 10-30 minutes (depends on data size and GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Stage 1: Compression Pretraining\n",
    "print('üöÄ Starting Stage 1: Compression Pretraining\\n')\n",
    "\n",
    "!torchrun --nproc_per_node={NUM_GPUS} \\\n",
    "    --master_port=29500 \\\n",
    "    -m openrlhf.cli.train_sft \\\n",
    "    --max_len {MAX_LEN} \\\n",
    "    --dataset {PRETRAIN_DATA} \\\n",
    "    --pretrain {MODEL_PATH} \\\n",
    "    --train_batch_size {TRAIN_BATCH_SIZE} \\\n",
    "    --micro_train_batch_size {MICRO_BATCH_SIZE} \\\n",
    "    --max_samples {MAX_SAMPLES} \\\n",
    "    --save_path {CHECKPOINT_DIR}/clara_stage1 \\\n",
    "    --save_steps -2 \\\n",
    "    --logging_steps 5 \\\n",
    "    --eval_steps -1 \\\n",
    "    --zero_stage 2 \\\n",
    "    --max_epochs {MAX_EPOCHS} \\\n",
    "    --bf16 \\\n",
    "    {FLASH_ATTN_FLAG} \\\n",
    "    --learning_rate {LEARNING_RATE} \\\n",
    "    --stage stage1 \\\n",
    "    --generation_top_k 1 \\\n",
    "    --qa_loss \\\n",
    "    --doc_max_length {DOC_MAX_LENGTH} \\\n",
    "    --compress_rate {COMPRESS_RATE} \\\n",
    "    --mse_loss \\\n",
    "    --gradient_checkpointing\n",
    "\n",
    "print('\\n‚úÖ Stage 1 completed!')\n",
    "print(f'Checkpoint saved to: {CHECKPOINT_DIR}/clara_stage1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Stage 1 Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify checkpoint\n",
    "!ls -lh {CHECKPOINT_DIR}/clara_stage1/\n",
    "!du -sh {CHECKPOINT_DIR}/clara_stage1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7Ô∏è‚É£ Stage 2: Instruction Tuning\n",
    "\n",
    "Fine-tune the compressor on instruction-following tasks.\n",
    "\n",
    "**What happens:**\n",
    "- Load Stage 1 checkpoint\n",
    "- Fine-tune on downstream QA tasks\n",
    "- Ensure compressed representations retain sufficient semantics\n",
    "\n",
    "**Expected time:** 10-30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Stage 2: Instruction Tuning\n",
    "print('üöÄ Starting Stage 2: Instruction Tuning\\n')\n",
    "\n",
    "!torchrun --nproc_per_node={NUM_GPUS} \\\n",
    "    --master_port=29500 \\\n",
    "    -m openrlhf.cli.train_sft \\\n",
    "    --max_len {MAX_LEN} \\\n",
    "    --dataset {INSTRUCTION_DATA} \\\n",
    "    --pretrain {MODEL_PATH} \\\n",
    "    --ckpt_path {CHECKPOINT_DIR}/clara_stage1 \\\n",
    "    --train_batch_size {TRAIN_BATCH_SIZE} \\\n",
    "    --micro_train_batch_size {MICRO_BATCH_SIZE} \\\n",
    "    --max_samples {MAX_SAMPLES} \\\n",
    "    --save_path {CHECKPOINT_DIR}/clara_stage2 \\\n",
    "    --save_steps -2 \\\n",
    "    --logging_steps 5 \\\n",
    "    --eval_steps -1 \\\n",
    "    --zero_stage 2 \\\n",
    "    --max_epochs {MAX_EPOCHS} \\\n",
    "    --bf16 \\\n",
    "    {FLASH_ATTN_FLAG} \\\n",
    "    --learning_rate {LEARNING_RATE} \\\n",
    "    --stage stage2 \\\n",
    "    --generation_top_k 1 \\\n",
    "    --doc_max_length {DOC_MAX_LENGTH} \\\n",
    "    --compress_rate {COMPRESS_RATE} \\\n",
    "    --gradient_checkpointing\n",
    "\n",
    "print('\\n‚úÖ Stage 2 completed!')\n",
    "print(f'Checkpoint saved to: {CHECKPOINT_DIR}/clara_stage2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify checkpoint\n",
    "!ls -lh {CHECKPOINT_DIR}/clara_stage2/\n",
    "!du -sh {CHECKPOINT_DIR}/clara_stage2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8Ô∏è‚É£ Stage 3: End-to-End Fine-tuning\n",
    "\n",
    "Jointly train reranker and generator.\n",
    "\n",
    "**What happens:**\n",
    "- Load Stage 2 checkpoint\n",
    "- Unify retrieval and generation in shared continuous space\n",
    "- Use differentiable top-k estimator\n",
    "- Train via single language modeling loss\n",
    "\n",
    "**Expected time:** 15-40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Stage 3: End-to-End Training\n",
    "print('üöÄ Starting Stage 3: End-to-End Fine-tuning\\n')\n",
    "\n",
    "!torchrun --nproc_per_node={NUM_GPUS} \\\n",
    "    --master_port=29500 \\\n",
    "    -m openrlhf.cli.train_sft \\\n",
    "    --max_len {MAX_LEN} \\\n",
    "    --dataset {END_TO_END_DATA} \\\n",
    "    --pretrain {MODEL_PATH} \\\n",
    "    --ckpt_path {CHECKPOINT_DIR}/clara_stage2 \\\n",
    "    --train_batch_size {TRAIN_BATCH_SIZE} \\\n",
    "    --micro_train_batch_size {MICRO_BATCH_SIZE} \\\n",
    "    --max_samples {MAX_SAMPLES} \\\n",
    "    --save_path {CHECKPOINT_DIR}/clara_stage3_final \\\n",
    "    --save_steps -2 \\\n",
    "    --logging_steps 5 \\\n",
    "    --eval_steps -1 \\\n",
    "    --zero_stage 2 \\\n",
    "    --max_epochs {MAX_EPOCHS} \\\n",
    "    --bf16 \\\n",
    "    {FLASH_ATTN_FLAG} \\\n",
    "    --learning_rate {LEARNING_RATE} \\\n",
    "    --stage stage3 \\\n",
    "    --generation_top_k 5 \\\n",
    "    --doc_max_length {DOC_MAX_LENGTH} \\\n",
    "    --compress_rate {COMPRESS_RATE} \\\n",
    "    --gradient_checkpointing\n",
    "\n",
    "print('\\n‚úÖ Stage 3 completed!')\n",
    "print(f'Final model saved to: {CHECKPOINT_DIR}/clara_stage3_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify final checkpoint\n",
    "!ls -lh {CHECKPOINT_DIR}/clara_stage3_final/\n",
    "!du -sh {CHECKPOINT_DIR}/clara_stage3_final/\n",
    "\n",
    "print('\\nüéâ Training pipeline completed!')\n",
    "print('\\nüìÅ All checkpoints:')\n",
    "!ls -lh {CHECKPOINT_DIR}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9Ô∏è‚É£ Model Inference Test\n",
    "\n",
    "Test the trained model with a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model for inference\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model_path = f'{CHECKPOINT_DIR}/clara_stage3_final'\n",
    "print(f'Loading model from: {model_path}')\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map='auto'\n",
    "    )\n",
    "    print('‚úÖ Model loaded successfully')\n",
    "    \n",
    "    # Test inference\n",
    "    test_query = \"What is CLaRa?\"\n",
    "    test_doc = \"CLaRa is a framework that bridges retrieval and generation with continuous latent reasoning.\"\n",
    "    \n",
    "    prompt = f\"Document: {test_doc}\\n\\nQuestion: {test_query}\\n\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f'\\nüìù Test Query: {test_query}')\n",
    "    print(f'ü§ñ Model Response:')\n",
    "    print(response)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error loading model: {e}')\n",
    "    print('This is expected if training was skipped or checkpoint format needs adjustment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîü Export and Save Model\n",
    "\n",
    "Download your trained model to local machine or save to Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Download to Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip archive of the final model\n",
    "!apt-get install -y zip\n",
    "!cd {CHECKPOINT_DIR} && zip -r clara_stage3_final.zip clara_stage3_final/\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "# files.download(f'{CHECKPOINT_DIR}/clara_stage3_final.zip')\n",
    "print(f'Model archived to: {CHECKPOINT_DIR}/clara_stage3_final.zip')\n",
    "print('Uncomment the download line above to download to your local machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Copy checkpoint to Drive\n",
    "# !cp -r {CHECKPOINT_DIR}/clara_stage3_final /content/drive/MyDrive/\n",
    "# print('‚úÖ Model saved to Google Drive')\n",
    "\n",
    "print('Uncomment the lines above to save to Google Drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Training Summary\n",
    "\n",
    "### Checkpoints Created:\n",
    "1. **Stage 1**: `/content/checkpoints/clara_stage1` - Compression pretraining\n",
    "2. **Stage 2**: `/content/checkpoints/clara_stage2` - Instruction tuning\n",
    "3. **Stage 3**: `/content/checkpoints/clara_stage3_final` - Final end-to-end model\n",
    "\n",
    "### Next Steps:\n",
    "1. **Evaluation**: Use the evaluation scripts in `scripts/evaluation_*.sh`\n",
    "2. **Fine-tuning**: Continue training with your own data\n",
    "3. **Deployment**: Export model for inference\n",
    "\n",
    "### Useful Resources:\n",
    "- üìÑ [Paper](https://arxiv.org/abs/2511.18659)\n",
    "- üíª [GitHub](https://github.com/apple/ml-clara)\n",
    "- ü§ó [HuggingFace Models](https://huggingface.co/probejie)\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Troubleshooting\n",
    "\n",
    "**Out of Memory (OOM):**\n",
    "- Reduce `TRAIN_BATCH_SIZE` and `MICRO_BATCH_SIZE`\n",
    "- Decrease `MAX_SAMPLES`\n",
    "- Use gradient checkpointing (already enabled)\n",
    "\n",
    "**Training too slow:**\n",
    "- Install flash-attn (see cell above)\n",
    "- Use A100 GPU instead of T4\n",
    "- Reduce data size for testing\n",
    "\n",
    "**Checkpoint loading errors:**\n",
    "- Verify checkpoint path exists\n",
    "- Check disk space\n",
    "- Ensure previous stage completed successfully\n",
    "\n",
    "---\n",
    "\n",
    "**Made with ‚ù§Ô∏è by the CLaRa Team**\n",
    "\n",
    "If you use this code, please cite:\n",
    "```bibtex\n",
    "@article{zhao2024clara,\n",
    "  title={CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning},\n",
    "  author={Zhao, Zhihao and others},\n",
    "  journal={arXiv preprint arXiv:2511.18659},\n",
    "  year={2024}\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}