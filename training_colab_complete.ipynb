{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ CLaRa Training on Colab\n",
    "\n",
    "[![Paper](https://img.shields.io/badge/Paper-Arxiv-green)](https://arxiv.org/abs/2511.18659)\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-ml--clara-blue)](https://github.com/apple/ml-clara)\n",
    "\n",
    "**CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning**\n",
    "\n",
    "This notebook provides a complete training pipeline for CLaRa model on Google Colab.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Training Pipeline\n",
    "\n",
    "1. **Environment Setup** - Check GPU, install dependencies\n",
    "2. **Code & Data Preparation** - Clone repository, prepare training data\n",
    "3. **Stage 1: Compression Pretraining** - Train compressor with KPCP framework\n",
    "4. **Stage 2: Instruction Tuning** - Fine-tune on instruction-following tasks\n",
    "5. **Stage 3: End-to-End Training** - Joint training of reranker and generator\n",
    "6. **Model Inference & Export** - Test model and save checkpoints\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Configuration\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- GPU: T4 (16GB) or better (A100 recommended)\n",
    "- RAM: High RAM (25GB+)\n",
    "- Runtime: GPU with High RAM\n",
    "\n",
    "**Training Settings:**\n",
    "- Base Model: `mistralai/Mistral-7B-Instruct-v0.2`\n",
    "- Compression Rate: 32x\n",
    "- Training Framework: OpenRLHF + DeepSpeed ZeRO-2\n",
    "- Batch Size: Adaptive based on available GPU memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1Ô∏è‚É£ Environment Setup\n",
    "\n",
    "Check GPU availability and system information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  2 13:50:30 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P0             46W /  400W |       5MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "============================================================\n",
      "PyTorch Version: 2.9.0+cu126\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.6\n",
      "GPU Device: NVIDIA A100-SXM4-40GB\n",
      "GPU Memory: 39.56 GB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU and CUDA\n",
    "!nvidia-smi\n",
    "print('\\n' + '='*60)\n",
    "import torch\n",
    "print(f'PyTorch Version: {torch.__version__}')\n",
    "print(f'CUDA Available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA Version: {torch.version.cuda}')\n",
    "    print(f'GPU Device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2Ô∏è‚É£ Install Dependencies\n",
    "\n",
    "Install required packages. This may take 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing core dependencies...\n",
      "‚úÖ Core packages installed\n",
      "\n",
      "üì¶ Fixing fsspec/gcsfs version conflict...\n",
      "‚úÖ gcsfs downgraded to 2024.6.1 (stable version)\n",
      "\n",
      "üì¶ Installing DeepSpeed...\n",
      "‚úÖ DeepSpeed 0.18.1 installed\n",
      "\n",
      "üì¶ Installing WandB (optional)...\n",
      "‚úÖ WandB installed\n",
      "\n",
      "üéâ Dependencies installation complete!\n",
      "CPU times: user 6.18 s, sys: 834 ms, total: 7.01 s\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Install core dependencies\n",
    "print('üì¶ Installing core dependencies...')\n",
    "\n",
    "# Install basic packages first\n",
    "!pip install -q accelerate==1.10.1 transformers==4.56.2 datasets==3.2.0 \\\n",
    "    peft==0.17.1 einops==0.8.1 sentencepiece==0.2.0 tiktoken==0.11.0\n",
    "\n",
    "print('‚úÖ Core packages installed')\n",
    "\n",
    "# Fix fsspec/gcsfs version conflict\n",
    "print('\\nüì¶ Fixing fsspec/gcsfs version conflict...')\n",
    "# Background: Colab comes with gcsfs 2025.3.0, which requires fsspec==2025.3.0\n",
    "# But datasets 3.2.0 requires fsspec<=2024.9.0 (incompatible!)\n",
    "# Solution: Use gcsfs 2024.6.1, a stable version compatible with fsspec 2024.9.0\n",
    "!pip install -q gcsfs==2024.6.1\n",
    "print('‚úÖ gcsfs downgraded to 2024.6.1 (stable version)')\n",
    "\n",
    "# Install DeepSpeed (may fail on some systems)\n",
    "print('\\nüì¶ Installing DeepSpeed...')\n",
    "try:\n",
    "    !pip install -q deepspeed==0.18.1\n",
    "    import deepspeed\n",
    "    print(f'‚úÖ DeepSpeed {deepspeed.__version__} installed')\n",
    "except Exception as e:\n",
    "    print(f'‚ö†Ô∏è  DeepSpeed installation failed: {e}')\n",
    "    print('   Training will continue without DeepSpeed optimizations')\n",
    "\n",
    "# Install WandB (optional)\n",
    "print('\\nüì¶ Installing WandB (optional)...')\n",
    "!pip install -q wandb==0.22.2\n",
    "print('‚úÖ WandB installed')\n",
    "\n",
    "print('\\nüéâ Dependencies installation complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Install Flash Attention (Recommended for ~15% speedup)\n",
    "\n",
    "**‚ö° Flash Attention Benefits:**\n",
    "- 10-15% faster training\n",
    "- Slightly lower memory usage\n",
    "\n",
    "**‚è±Ô∏è Installation Time:**\n",
    "- **Precompiled wheel (recommended)**: 2-3 minutes\n",
    "- **From source (not recommended)**: 20-40 minutes (may timeout in Colab)\n",
    "\n",
    "**üí° Recommendation:**\n",
    "- Enable for production/large-scale training\n",
    "- Skip for quick testing (code auto-falls back to standard attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è  Skipping Flash Attention installation\n",
      "   Training will use standard eager attention (fully functional)\n",
      "   To enable flash_attn: Set INSTALL_FLASH_ATTN = True above\n",
      "\n",
      "üéØ Flash Attention Status: DISABLED (using eager attention)\n",
      "CPU times: user 114 ¬µs, sys: 0 ns, total: 114 ¬µs\n",
      "Wall time: 110 ¬µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# ‚öôÔ∏è Configuration: Set to True to install flash_attn\n",
    "INSTALL_FLASH_ATTN = True  # Change to True if you want ~15% speedup\n",
    "\n",
    "if INSTALL_FLASH_ATTN:\n",
    "    print('‚ö° Installing Flash Attention...\\n')\n",
    "    \n",
    "    try:\n",
    "        # Detect environment\n",
    "        cuda_version = torch.version.cuda\n",
    "        python_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "        torch_version = torch.__version__.split('+')[0]\n",
    "        \n",
    "        print(f'üìã Environment Info:')\n",
    "        print(f'  - CUDA: {cuda_version}')\n",
    "        print(f'  - Python: {python_version}')\n",
    "        print(f'  - PyTorch: {torch_version}')\n",
    "        print(f'  - Platform: linux_x86_64\\n')\n",
    "        \n",
    "        print('üì• Installing flash-attn from PyPI...')\n",
    "        print('   Strategy: Try precompiled wheel first, skip compilation if unavailable')\n",
    "        print('   This should take 1-3 minutes...\\n')\n",
    "        \n",
    "        # Strategy: Try to install from PyPI\n",
    "        # - If precompiled wheel exists: installs quickly (~30 seconds)\n",
    "        # - If not: --no-build-isolation prevents compilation (fails fast)\n",
    "        import subprocess\n",
    "        result = subprocess.run(\n",
    "            ['pip', 'install', 'flash-attn', '--no-build-isolation', '-q'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=180  # 3 minute timeout\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            # Verify installation\n",
    "            import flash_attn\n",
    "            print(f'‚úÖ Flash Attention {flash_attn.__version__} installed successfully!')\n",
    "            print('   Training will use flash_attention_2 for ~15% speedup\\n')\n",
    "            USE_FLASH_ATTN = True\n",
    "        else:\n",
    "            raise Exception(\"Installation failed (no precompiled wheel available)\")\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f'‚ö†Ô∏è  Flash Attention installation timed out (>3 min)')\n",
    "        print('   This usually means it tried to compile from source')\n",
    "        print('   Skipping to avoid long wait times...\\n')\n",
    "        USE_FLASH_ATTN = False\n",
    "    except Exception as e:\n",
    "        print(f'‚ö†Ô∏è  Flash Attention installation failed')\n",
    "        print(f'   Reason: {str(e)}')\n",
    "        print('   This is OK - training will automatically fall back to standard attention')\n",
    "        print('   Performance impact: ~10-15% slower (minor for small models)\\n')\n",
    "        USE_FLASH_ATTN = False\n",
    "else:\n",
    "    print('‚è≠Ô∏è  Skipping Flash Attention installation')\n",
    "    print('   Training will use standard eager attention (fully functional)')\n",
    "    print('   To enable flash_attn: Set INSTALL_FLASH_ATTN = True above\\n')\n",
    "    USE_FLASH_ATTN = False\n",
    "\n",
    "print(f'üéØ Flash Attention Status: {\"ENABLED ‚úÖ\" if USE_FLASH_ATTN else \"DISABLED (using eager attention)\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3Ô∏è‚É£ Download Code and Data\n",
    "\n",
    "Clone CLaRa repository and OpenRLHF framework.\n",
    "\n",
    "**Note:** This notebook uses a custom fork with the following fixes:\n",
    "- ‚úÖ Flash Attention made optional (with fallback implementations)\n",
    "- ‚úÖ Dependency conflicts resolved (fsspec, gcsfs)\n",
    "- ‚úÖ Colab-optimized training pipeline\n",
    "\n",
    "The custom fork ensures training works smoothly without requiring flash_attn installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Clone CLaRa repository (with flash_attn fallback fixes and complete OpenRLHF integration)\n",
    "if not os.path.exists('ml-clara'):\n",
    "    # Use custom fork with fixes instead of official repo\n",
    "    print('üì• Cloning CLaRa repository...')\n",
    "    !git clone https://github.com/xucheng/ml-clara-rag.git ml-clara\n",
    "    print('‚úÖ CLaRa repository cloned (with fixes)')\n",
    "    print('   - Includes flash_attn fallback implementation')\n",
    "    print('   - Includes complete OpenRLHF framework')\n",
    "else:\n",
    "    print('‚úÖ CLaRa repository already exists')\n",
    "    # Pull latest changes if repository exists\n",
    "    print('üì• Pulling latest changes...')\n",
    "    !cd ml-clara && git pull origin main\n",
    "    print('‚úÖ Repository updated')\n",
    "\n",
    "# Verify OpenRLHF is included\n",
    "print('\\nüì¶ Verifying OpenRLHF framework...')\n",
    "if os.path.exists('ml-clara/openrlhf'):\n",
    "    # Use Python's glob instead of shell command for better compatibility\n",
    "    py_files = glob.glob('ml-clara/openrlhf/**/*.py', recursive=True)\n",
    "    file_count = len(py_files)\n",
    "    print(f'‚úÖ OpenRLHF framework ready ({file_count} Python files)')\n",
    "    \n",
    "    # Check critical files\n",
    "    critical_files = [\n",
    "        'ml-clara/openrlhf/models/ring_attn_utils.py',\n",
    "        'ml-clara/openrlhf/cli/train_sft.py',\n",
    "        'ml-clara/openrlhf/models/modeling_clara.py'\n",
    "    ]\n",
    "    missing_files = [f for f in critical_files if not os.path.exists(f)]\n",
    "    \n",
    "    if missing_files:\n",
    "        print('‚ö†Ô∏è  Warning: Some critical files are missing:')\n",
    "        for f in missing_files:\n",
    "            print(f'   - {f}')\n",
    "    else:\n",
    "        print('‚úÖ All critical files present')\n",
    "else:\n",
    "    print('‚ùå OpenRLHF not found - please check repository')\n",
    "    print('   This may indicate a git clone failure.')\n",
    "\n",
    "# Change to project directory\n",
    "%cd ml-clara\n",
    "print(f'üìÇ Current directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è Fix: Patch sft_dataset.py to support 'gold_answer' field\n",
    "# -----------------------------------------------------------\n",
    "import os\n",
    "\n",
    "# We are already in 'ml-clara' directory\n",
    "file_path = \"openrlhf/datasets/sft_dataset.py\"\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Check if the patch is already applied\n",
    "    if 'elif \"gold_answer\" in data and isinstance(data[\\'gold_answer\\'], str):' not in content:\n",
    "        print(\"üîß Patching sft_dataset.py...\")\n",
    "        \n",
    "        # Exact string matching based on the file content\n",
    "        search_str = '    if \"answer\" in data and isinstance(data[\\'answer\\'], str):\\n        answers = data[\\'answer\\']\\n    elif \"answers\" in data and isinstance(data[\\'answers\\'], list):\\n        answers = data[\\'answers\\']'\n",
    "        \n",
    "        replace_str = '    if \"answer\" in data and isinstance(data[\\'answer\\'], str):\\n        answers = data[\\'answer\\']\\n    elif \"gold_answer\" in data and isinstance(data[\\'gold_answer\\'], str):\\n        answers = data[\\'gold_answer\\']\\n    elif \"answers\" in data and isinstance(data[\\'answers\\'], list):\\n        answers = data[\\'answers\\']'\n",
    "\n",
    "        if search_str in content:\n",
    "            new_content = content.replace(search_str, replace_str)\n",
    "            with open(file_path, \"w\") as f:\n",
    "                f.write(new_content)\n",
    "            print(\"‚úÖ Patch applied successfully!\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Could not find exact code pattern to patch. Please check sft_dataset.py manually.\")\n",
    "    else:\n",
    "        print(\"‚úÖ File already patched.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è File not found: {file_path}. Make sure you are in the correct directory (ml-clara).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4Ô∏è‚É£ Data Preparation\n",
    "\n",
    "You have two options:\n",
    "1. **Use example data** (provided in repository) - Quick start\n",
    "2. **Upload your own data** - For custom training\n",
    "\n",
    "### Data Format\n",
    "\n",
    "**Stage 1 (Pretraining)**: `pretrain_data.jsonl`\n",
    "```json\n",
    "{\"data_type\": \"qa\", \"question\": [\"Q1\"], \"answers\": [\"A1\"], \"docs\": [\"doc1\"]}\n",
    "```\n",
    "\n",
    "**Stage 2 (Instruction Tuning)**: `instruction_data.jsonl`\n",
    "```json\n",
    "{\"question\": \"Q1\", \"docs\": [\"doc1\"], \"gold_answer\": \"A1\"}\n",
    "```\n",
    "\n",
    "**Stage 3 (End-to-End)**: `end_to_end_data.jsonl` (same as Stage 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check example data\n",
    "!ls -lh example/*.jsonl\n",
    "print('\\nüìä Example data statistics:')\n",
    "!wc -l example/*.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Use Example Data (Recommended for first run)\n",
    "\n",
    "The repository includes small example datasets for quick testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use example data (already in repository)\n",
    "DATA_MODE = 'example'  # or 'custom'\n",
    "\n",
    "if DATA_MODE == 'example':\n",
    "    PRETRAIN_DATA = 'example/pretrain_data.jsonl'\n",
    "    INSTRUCTION_DATA = 'example/instruction_data.jsonl'\n",
    "    END_TO_END_DATA = 'example/end_to_end_data.jsonl'\n",
    "    print('‚úÖ Using example data from repository')\n",
    "    print(f'  - Pretraining: {PRETRAIN_DATA}')\n",
    "    print(f'  - Instruction: {INSTRUCTION_DATA}')\n",
    "    print(f'  - End-to-End: {END_TO_END_DATA}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Load from Google Drive\n",
    "\n",
    "Mount Google Drive and use data files stored there.\n",
    "\n",
    "**Example paths in Google Drive:**\n",
    "- `/content/drive/MyDrive/Colab Notebooks/data/ml-clara/pretrain_data.jsonl`\n",
    "- `/content/drive/MyDrive/data/ml-clara/instruction_data.jsonl`\n",
    "\n",
    "Run the cell below to mount Drive and set paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Detect environment\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "if IS_COLAB:\n",
    "    # Mount Google Drive\n",
    "    print('üìÇ Mounting Google Drive...')\n",
    "    drive.mount('/content/drive')\n",
    "    print('‚úÖ Google Drive mounted at /content/drive')\n",
    "    \n",
    "    # Modify these paths to match your Drive folder structure\n",
    "    # Example: If your files are in \"Colab Notebooks/data/ml-clara/\"\n",
    "    DRIVE_BASE = '/content/drive/MyDrive/Colab Notebooks/data/ml-clara'\n",
    "    \n",
    "    PRETRAIN_DATA = f'{DRIVE_BASE}/pretrain_data.jsonl'\n",
    "    INSTRUCTION_DATA = f'{DRIVE_BASE}/instruction_data.jsonl'\n",
    "    END_TO_END_DATA = f'{DRIVE_BASE}/end_to_end_data.jsonl'\n",
    "    \n",
    "    print(f'\\nüìÅ Looking for data in: {DRIVE_BASE}')\n",
    "    \n",
    "    # Verify files exist\n",
    "    all_found = True\n",
    "    for name, path in [('Pretrain', PRETRAIN_DATA),\n",
    "                       ('Instruction', INSTRUCTION_DATA),\n",
    "                       ('End-to-End', END_TO_END_DATA)]:\n",
    "        if os.path.exists(path):\n",
    "            file_size = os.path.getsize(path) / 1024  # KB\n",
    "            print(f'‚úÖ {name}: {path} ({file_size:.1f} KB)')\n",
    "        else:\n",
    "            print(f'‚ùå {name}: {path} (NOT FOUND)')\n",
    "            all_found = False\n",
    "    \n",
    "    if all_found:\n",
    "        DATA_MODE = 'drive'\n",
    "        print(f'\\n‚úÖ All data files found in Google Drive')\n",
    "    else:\n",
    "        print(f'\\n‚ö†Ô∏è  Some files not found. Please check:')\n",
    "        print(f'   1. Files are uploaded to: {DRIVE_BASE}')\n",
    "        print(f'   2. Folder path is correct (including spaces)')\n",
    "        print(f'   3. File names match exactly')\n",
    "        print(f'\\nüí° To fix: Update DRIVE_BASE path in this cell')\n",
    "else:\n",
    "    print('‚ö†Ô∏è  Not in Google Colab environment')\n",
    "    print('This cell is designed for Google Colab with Drive mounting')\n",
    "    print('Use Option A (example data) or Option C (local paths) instead')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option C: Upload Files or Use Local Paths\n",
    "\n",
    "This cell automatically detects your environment:\n",
    "\n",
    "**In Google Colab:**\n",
    "- Uses the upload widget (`files.upload()`)\n",
    "- Simply run the cell and select files when prompted\n",
    "\n",
    "**In Local/VS Code:**\n",
    "- Uses file paths instead\n",
    "- Modify the paths to point to your local data files\n",
    "\n",
    "Run the cell to load custom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect environment\n",
    "try:\n",
    "    from google.colab import files\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "\n",
    "print(f'Environment: {\"Google Colab\" if IS_COLAB else \"Local/VS Code\"}')\n",
    "\n",
    "# Option 1: For Google Colab - Use upload widget\n",
    "if IS_COLAB:\n",
    "    print('\\nüì§ Upload your custom data files:')\n",
    "    \n",
    "    print('\\n1Ô∏è‚É£ Upload pretrain_data.jsonl:')\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    print('\\n2Ô∏è‚É£ Upload instruction_data.jsonl:')\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    print('\\n3Ô∏è‚É£ Upload end_to_end_data.jsonl:')\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Move to data directory\n",
    "    !mkdir -p data\n",
    "    !mv pretrain_data.jsonl instruction_data.jsonl end_to_end_data.jsonl data/\n",
    "    \n",
    "    DATA_MODE = 'custom'\n",
    "    PRETRAIN_DATA = 'data/pretrain_data.jsonl'\n",
    "    INSTRUCTION_DATA = 'data/instruction_data.jsonl'\n",
    "    END_TO_END_DATA = 'data/end_to_end_data.jsonl'\n",
    "    print('\\n‚úÖ Custom data uploaded')\n",
    "\n",
    "# Option 2: For Local/VS Code - Specify file paths\n",
    "else:\n",
    "    print('\\nüìÅ Using local file paths')\n",
    "    print('Please modify the paths below to point to your data files:\\n')\n",
    "    \n",
    "    # Modify these paths to your actual data locations\n",
    "    PRETRAIN_DATA = 'example/pretrain_data.jsonl'  # Change this\n",
    "    INSTRUCTION_DATA = 'example/instruction_data.jsonl'  # Change this\n",
    "    END_TO_END_DATA = 'example/end_to_end_data.jsonl'  # Change this\n",
    "    \n",
    "    # Verify files exist\n",
    "    missing_files = []\n",
    "    for name, path in [('Pretrain', PRETRAIN_DATA), \n",
    "                       ('Instruction', INSTRUCTION_DATA), \n",
    "                       ('End-to-End', END_TO_END_DATA)]:\n",
    "        if os.path.exists(path):\n",
    "            print(f'‚úÖ {name}: {path}')\n",
    "        else:\n",
    "            print(f'‚ùå {name}: {path} (NOT FOUND)')\n",
    "            missing_files.append(path)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f'\\n‚ö†Ô∏è  Warning: {len(missing_files)} file(s) not found')\n",
    "        print('Please update the file paths in this cell or use example data')\n",
    "    else:\n",
    "        DATA_MODE = 'custom'\n",
    "        print('\\n‚úÖ All custom data files found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5Ô∏è‚É£ Training Configuration\n",
    "\n",
    "Set up training parameters. Adjust based on your GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Detect GPU memory and set batch sizes\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f'GPU Memory: {gpu_memory:.1f} GB')\n",
    "    \n",
    "    if gpu_memory < 20:  # T4 (16GB)\n",
    "        TRAIN_BATCH_SIZE = 32\n",
    "        MICRO_BATCH_SIZE = 1\n",
    "        NUM_GPUS = 1\n",
    "        MAX_SAMPLES = 200\n",
    "        print('‚öôÔ∏è Using T4 config (16GB)')\n",
    "    elif gpu_memory < 50:  # V100 or A100-40GB\n",
    "        TRAIN_BATCH_SIZE = 64\n",
    "        MICRO_BATCH_SIZE = 2\n",
    "        NUM_GPUS = 1\n",
    "        MAX_SAMPLES = 500\n",
    "        print('‚öôÔ∏è Using V100/A100-40GB config')\n",
    "    else:  # A100-80GB\n",
    "        TRAIN_BATCH_SIZE = 128\n",
    "        MICRO_BATCH_SIZE = 2\n",
    "        NUM_GPUS = 1\n",
    "        MAX_SAMPLES = 1000\n",
    "        print('‚öôÔ∏è Using A100-80GB config')\n",
    "else:\n",
    "    raise RuntimeError('‚ùå No GPU available. Please enable GPU runtime.')\n",
    "\n",
    "# Model and checkpoint paths\n",
    "MODEL_PATH = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "CHECKPOINT_DIR = '/content/checkpoints'\n",
    "\n",
    "# Training settings\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_EPOCHS = 1\n",
    "COMPRESS_RATE = 32\n",
    "DOC_MAX_LENGTH = 256\n",
    "MAX_LEN = 2048\n",
    "\n",
    "# Flash attention flag\n",
    "FLASH_ATTN_FLAG = '--flash_attn' if USE_FLASH_ATTN else ''\n",
    "\n",
    "print(f'\\nüìù Training Configuration:')\n",
    "print(f'  Model: {MODEL_PATH}')\n",
    "print(f'  Batch Size: {TRAIN_BATCH_SIZE}')\n",
    "print(f'  Micro Batch Size: {MICRO_BATCH_SIZE}')\n",
    "print(f'  Max Samples: {MAX_SAMPLES}')\n",
    "print(f'  Learning Rate: {LEARNING_RATE}')\n",
    "print(f'  Compress Rate: {COMPRESS_RATE}x')\n",
    "print(f'  Flash Attention: {USE_FLASH_ATTN}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6Ô∏è‚É£ Stage 1: Compression Pretraining\n",
    "\n",
    "Train the compressor using KPCP framework with QA pairs and paraphrases.\n",
    "\n",
    "**What happens:**\n",
    "- Compress documents into continuous latent representations\n",
    "- Learn semantic compression through QA-based supervision\n",
    "- Support compression rates from 1x to 256x\n",
    "\n",
    "**Expected time:** 10-30 minutes (depends on data size and GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Stage 1: Compression Pretraining\n",
    "print('üöÄ Starting Stage 1: Compression Pretraining\\n')\n",
    "\n",
    "!torchrun --nproc_per_node={NUM_GPUS} \\\n",
    "    --master_port=29500 \\\n",
    "    -m openrlhf.cli.train_sft \\\n",
    "    --max_len {MAX_LEN} \\\n",
    "    --dataset \"{PRETRAIN_DATA}\" \\\n",
    "    --pretrain \"{MODEL_PATH}\" \\\n",
    "    --train_batch_size {TRAIN_BATCH_SIZE} \\\n",
    "    --micro_train_batch_size {MICRO_BATCH_SIZE} \\\n",
    "    --max_samples {MAX_SAMPLES} \\\n",
    "    --save_path \"{CHECKPOINT_DIR}/clara_stage1\" \\\n",
    "    --save_steps -2 \\\n",
    "    --logging_steps 5 \\\n",
    "    --eval_steps -1 \\\n",
    "    --zero_stage 2 \\\n",
    "    --max_epochs {MAX_EPOCHS} \\\n",
    "    --bf16 \\\n",
    "    {FLASH_ATTN_FLAG} \\\n",
    "    --learning_rate {LEARNING_RATE} \\\n",
    "    --stage stage1 \\\n",
    "    --generation_top_k 1 \\\n",
    "    --qa_loss \\\n",
    "    --doc_max_length {DOC_MAX_LENGTH} \\\n",
    "    --compress_rate {COMPRESS_RATE} \\\n",
    "    --mse_loss \\\n",
    "    --gradient_checkpointing\n",
    "\n",
    "print('\\n‚úÖ Stage 1 completed!')\n",
    "print(f'Checkpoint saved to: {CHECKPOINT_DIR}/clara_stage1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Stage 1 Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify checkpoint\n",
    "!ls -lh {CHECKPOINT_DIR}/clara_stage1/\n",
    "!du -sh {CHECKPOINT_DIR}/clara_stage1/"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import torch\nimport gc\nimport subprocess\nimport time\nimport os\nimport signal\n\n# üßπ Clean up GPU memory and training processes\nprint('üßπ Cleaning up GPU memory and processes...\\n')\n\n# Step 1: Show memory BEFORE cleanup\nprint('üìä Memory status BEFORE cleanup:')\nresult = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.free,memory.total', \n                        '--format=csv,noheader,nounits'], \n                       capture_output=True, text=True, check=False)\nif result.stdout:\n    used, free, total = result.stdout.strip().split(',')\n    print(f'  GPU Memory: {used.strip()} MB used / {total.strip()} MB total')\n    print(f'  Free: {free.strip()} MB\\n')\n\n# Step 2: Kill any remaining Python training processes\nprint('üî™ Killing remaining training processes...')\ntry:\n    # Get current process PID to avoid killing ourselves\n    current_pid = os.getpid()\n    \n    # Find all python3 processes\n    ps_result = subprocess.run(['ps', 'aux'], capture_output=True, text=True, check=False)\n    python_procs = []\n    \n    for line in ps_result.stdout.split('\\n'):\n        if 'python' in line.lower() and 'torchrun' not in line and str(current_pid) not in line:\n            parts = line.split()\n            if len(parts) > 1:\n                try:\n                    pid = int(parts[1])\n                    if pid != current_pid and pid != os.getppid():\n                        python_procs.append(pid)\n                except (ValueError, IndexError):\n                    pass\n    \n    # Kill training processes\n    killed_count = 0\n    for pid in python_procs:\n        try:\n            os.kill(pid, signal.SIGKILL)\n            killed_count += 1\n        except (ProcessLookupError, PermissionError):\n            pass\n    \n    if killed_count > 0:\n        print(f'  ‚úì Killed {killed_count} training process(es)')\n        time.sleep(3)  # Wait for processes to fully terminate\n    else:\n        print('  ‚úì No lingering training processes found')\n        \nexcept Exception as e:\n    print(f'  ‚ö† Error cleaning processes: {e}')\n\n# Step 3: PyTorch memory cleanup\nprint('\\nüßπ Cleaning PyTorch memory...')\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    torch.cuda.reset_peak_memory_stats()\nprint('  ‚úì PyTorch memory cleared')\n\n# Wait a bit for everything to settle\ntime.sleep(2)\n\n# Step 4: Show memory AFTER cleanup\nprint('\\nüìä Memory status AFTER cleanup:')\nresult = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.free,memory.total', \n                        '--format=csv,noheader,nounits'], \n                       capture_output=True, text=True, check=False)\nif result.stdout:\n    used, free, total = result.stdout.strip().split(',')\n    print(f'  GPU Memory: {used.strip()} MB used / {total.strip()} MB total')\n    print(f'  Free: {free.strip()} MB')\n    \n    # Calculate freed memory\n    try:\n        freed = int(free.strip())\n        total_mem = int(total.strip())\n        if freed > total_mem * 0.7:  # More than 70% free\n            print(f'  ‚úÖ Good! {freed/1024:.1f} GB free for next stage')\n        else:\n            print(f'  ‚ö†Ô∏è Warning: Only {freed/1024:.1f} GB free - may need Runtime restart')\n    except:\n        pass\n\nprint('\\n‚úÖ Cleanup completed!\\n')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7Ô∏è‚É£ Stage 2: Instruction Tuning\n",
    "\n",
    "Fine-tune the compressor on instruction-following tasks.\n",
    "\n",
    "**What happens:**\n",
    "- Load Stage 1 checkpoint\n",
    "- Fine-tune on downstream QA tasks\n",
    "- Ensure compressed representations retain sufficient semantics\n",
    "\n",
    "**Expected time:** 10-30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Stage 2: Instruction Tuning\n",
    "print('üöÄ Starting Stage 2: Instruction Tuning\\n')\n",
    "\n",
    "!torchrun --nproc_per_node={NUM_GPUS} \\\n",
    "    --master_port=29500 \\\n",
    "    -m openrlhf.cli.train_sft \\\n",
    "    --max_len {MAX_LEN} \\\n",
    "    --dataset \"{INSTRUCTION_DATA}\" \\\n",
    "    --pretrain \"{MODEL_PATH}\" \\\n",
    "    --ckpt_path \"{CHECKPOINT_DIR}/clara_stage1\" \\\n",
    "    --train_batch_size {TRAIN_BATCH_SIZE} \\\n",
    "    --micro_train_batch_size {MICRO_BATCH_SIZE} \\\n",
    "    --max_samples {MAX_SAMPLES} \\\n",
    "    --save_path \"{CHECKPOINT_DIR}/clara_stage2\" \\\n",
    "    --save_steps -2 \\\n",
    "    --logging_steps 5 \\\n",
    "    --eval_steps -1 \\\n",
    "    --zero_stage 2 \\\n",
    "    --max_epochs {MAX_EPOCHS} \\\n",
    "    --bf16 \\\n",
    "    {FLASH_ATTN_FLAG} \\\n",
    "    --learning_rate {LEARNING_RATE} \\\n",
    "    --stage stage2 \\\n",
    "    --generation_top_k 1 \\\n",
    "    --doc_max_length {DOC_MAX_LENGTH} \\\n",
    "    --compress_rate {COMPRESS_RATE} \\\n",
    "    --gradient_checkpointing\n",
    "\n",
    "print('\\n‚úÖ Stage 2 completed!')\n",
    "print(f'Checkpoint saved to: {CHECKPOINT_DIR}/clara_stage2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify checkpoint\n",
    "!ls -lh {CHECKPOINT_DIR}/clara_stage2/\n",
    "!du -sh {CHECKPOINT_DIR}/clara_stage2/"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import torch\nimport gc\nimport subprocess\nimport time\nimport os\nimport signal\n\n# üßπ Clean up GPU memory and training processes\nprint('üßπ Cleaning up GPU memory and processes...\\n')\n\n# Step 1: Show memory BEFORE cleanup\nprint('üìä Memory status BEFORE cleanup:')\nresult = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.free,memory.total', \n                        '--format=csv,noheader,nounits'], \n                       capture_output=True, text=True, check=False)\nif result.stdout:\n    used, free, total = result.stdout.strip().split(',')\n    print(f'  GPU Memory: {used.strip()} MB used / {total.strip()} MB total')\n    print(f'  Free: {free.strip()} MB\\n')\n\n# Step 2: Kill any remaining Python training processes\nprint('üî™ Killing remaining training processes...')\ntry:\n    # Get current process PID to avoid killing ourselves\n    current_pid = os.getpid()\n    \n    # Find all python3 processes\n    ps_result = subprocess.run(['ps', 'aux'], capture_output=True, text=True, check=False)\n    python_procs = []\n    \n    for line in ps_result.stdout.split('\\n'):\n        if 'python' in line.lower() and 'torchrun' not in line and str(current_pid) not in line:\n            parts = line.split()\n            if len(parts) > 1:\n                try:\n                    pid = int(parts[1])\n                    if pid != current_pid and pid != os.getppid():\n                        python_procs.append(pid)\n                except (ValueError, IndexError):\n                    pass\n    \n    # Kill training processes\n    killed_count = 0\n    for pid in python_procs:\n        try:\n            os.kill(pid, signal.SIGKILL)\n            killed_count += 1\n        except (ProcessLookupError, PermissionError):\n            pass\n    \n    if killed_count > 0:\n        print(f'  ‚úì Killed {killed_count} training process(es)')\n        time.sleep(3)  # Wait for processes to fully terminate\n    else:\n        print('  ‚úì No lingering training processes found')\n        \nexcept Exception as e:\n    print(f'  ‚ö† Error cleaning processes: {e}')\n\n# Step 3: PyTorch memory cleanup\nprint('\\nüßπ Cleaning PyTorch memory...')\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    torch.cuda.synchronize()\n    torch.cuda.reset_peak_memory_stats()\nprint('  ‚úì PyTorch memory cleared')\n\n# Wait a bit for everything to settle\ntime.sleep(2)\n\n# Step 4: Show memory AFTER cleanup\nprint('\\nüìä Memory status AFTER cleanup:')\nresult = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.free,memory.total', \n                        '--format=csv,noheader,nounits'], \n                       capture_output=True, text=True, check=False)\nif result.stdout:\n    used, free, total = result.stdout.strip().split(',')\n    print(f'  GPU Memory: {used.strip()} MB used / {total.strip()} MB total')\n    print(f'  Free: {free.strip()} MB')\n    \n    # Calculate freed memory\n    try:\n        freed = int(free.strip())\n        total_mem = int(total.strip())\n        if freed > total_mem * 0.7:  # More than 70% free\n            print(f'  ‚úÖ Good! {freed/1024:.1f} GB free for next stage')\n        else:\n            print(f'  ‚ö†Ô∏è Warning: Only {freed/1024:.1f} GB free - may need Runtime restart')\n    except:\n        pass\n\nprint('\\n‚úÖ Cleanup completed!\\n')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8Ô∏è‚É£ Stage 3: End-to-End Fine-tuning\n",
    "\n",
    "Jointly train reranker and generator.\n",
    "\n",
    "**What happens:**\n",
    "- Load Stage 2 checkpoint\n",
    "- Unify retrieval and generation in shared continuous space\n",
    "- Use differentiable top-k estimator\n",
    "- Train via single language modeling loss\n",
    "\n",
    "**Expected time:** 15-40 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Stage 3: End-to-End Training\n",
    "print('üöÄ Starting Stage 3: End-to-End Fine-tuning\\n')\n",
    "\n",
    "!torchrun --nproc_per_node={NUM_GPUS} \\\n",
    "    --master_port=29500 \\\n",
    "    -m openrlhf.cli.train_sft \\\n",
    "    --max_len {MAX_LEN} \\\n",
    "    --dataset \"{END_TO_END_DATA}\" \\\n",
    "    --pretrain \"{MODEL_PATH}\" \\\n",
    "    --ckpt_path \"{CHECKPOINT_DIR}/clara_stage2\" \\\n",
    "    --train_batch_size {TRAIN_BATCH_SIZE} \\\n",
    "    --micro_train_batch_size {MICRO_BATCH_SIZE} \\\n",
    "    --max_samples {MAX_SAMPLES} \\\n",
    "    --save_path \"{CHECKPOINT_DIR}/clara_stage3_final\" \\\n",
    "    --save_steps -2 \\\n",
    "    --logging_steps 5 \\\n",
    "    --eval_steps -1 \\\n",
    "    --zero_stage 2 \\\n",
    "    --max_epochs {MAX_EPOCHS} \\\n",
    "    --bf16 \\\n",
    "    {FLASH_ATTN_FLAG} \\\n",
    "    --learning_rate {LEARNING_RATE} \\\n",
    "    --stage stage3 \\\n",
    "    --generation_top_k 5 \\\n",
    "    --doc_max_length {DOC_MAX_LENGTH} \\\n",
    "    --compress_rate {COMPRESS_RATE} \\\n",
    "    --gradient_checkpointing\n",
    "\n",
    "print('\\n‚úÖ Stage 3 completed!')\n",
    "print(f'Final model saved to: {CHECKPOINT_DIR}/clara_stage3_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify final checkpoint\n",
    "!ls -lh {CHECKPOINT_DIR}/clara_stage3_final/\n",
    "!du -sh {CHECKPOINT_DIR}/clara_stage3_final/\n",
    "\n",
    "print('\\nüéâ Training pipeline completed!')\n",
    "print('\\nüìÅ All checkpoints:')\n",
    "!ls -lh {CHECKPOINT_DIR}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9Ô∏è‚É£ Model Inference Test\n",
    "\n",
    "Test the trained model with a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model for inference\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model_path = f'{CHECKPOINT_DIR}/clara_stage3_final'\n",
    "print(f'Loading model from: {model_path}')\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map='auto'\n",
    "    )\n",
    "    print('‚úÖ Model loaded successfully')\n",
    "    \n",
    "    # Test inference\n",
    "    test_query = \"What is CLaRa?\"\n",
    "    test_doc = \"CLaRa is a framework that bridges retrieval and generation with continuous latent reasoning.\"\n",
    "    \n",
    "    prompt = f\"Document: {test_doc}\\n\\nQuestion: {test_query}\\n\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f'\\nüìù Test Query: {test_query}')\n",
    "    print(f'ü§ñ Model Response:')\n",
    "    print(response)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error loading model: {e}')\n",
    "    print('This is expected if training was skipped or checkpoint format needs adjustment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîü Export and Save Model\n",
    "\n",
    "Download your trained model to local machine or save to Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Download to Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip archive of the final model\n",
    "!apt-get install -y zip\n",
    "!cd {CHECKPOINT_DIR} && zip -r clara_stage3_final.zip clara_stage3_final/\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "# files.download(f'{CHECKPOINT_DIR}/clara_stage3_final.zip')\n",
    "print(f'Model archived to: {CHECKPOINT_DIR}/clara_stage3_final.zip')\n",
    "print('Uncomment the download line above to download to your local machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Copy checkpoint to Drive\n",
    "# !cp -r {CHECKPOINT_DIR}/clara_stage3_final /content/drive/MyDrive/\n",
    "# print('‚úÖ Model saved to Google Drive')\n",
    "\n",
    "print('Uncomment the lines above to save to Google Drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Training Summary\n",
    "\n",
    "### Checkpoints Created:\n",
    "1. **Stage 1**: `/content/checkpoints/clara_stage1` - Compression pretraining\n",
    "2. **Stage 2**: `/content/checkpoints/clara_stage2` - Instruction tuning\n",
    "3. **Stage 3**: `/content/checkpoints/clara_stage3_final` - Final end-to-end model\n",
    "\n",
    "### Next Steps:\n",
    "1. **Evaluation**: Use the evaluation scripts in `scripts/evaluation_*.sh`\n",
    "2. **Fine-tuning**: Continue training with your own data\n",
    "3. **Deployment**: Export model for inference\n",
    "\n",
    "### Useful Resources:\n",
    "- üìÑ [Paper](https://arxiv.org/abs/2511.18659)\n",
    "- üíª [GitHub](https://github.com/apple/ml-clara)\n",
    "- ü§ó [HuggingFace Models](https://huggingface.co/probejie)\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Troubleshooting\n",
    "\n",
    "**Out of Memory (OOM):**\n",
    "- Reduce `TRAIN_BATCH_SIZE` and `MICRO_BATCH_SIZE`\n",
    "- Decrease `MAX_SAMPLES`\n",
    "- Use gradient checkpointing (already enabled)\n",
    "\n",
    "**Training too slow:**\n",
    "- Install flash-attn (see cell above)\n",
    "- Use A100 GPU instead of T4\n",
    "- Reduce data size for testing\n",
    "\n",
    "**Checkpoint loading errors:**\n",
    "- Verify checkpoint path exists\n",
    "- Check disk space\n",
    "- Ensure previous stage completed successfully\n",
    "\n",
    "---\n",
    "\n",
    "**Made with ‚ù§Ô∏è by the CLaRa Team**\n",
    "\n",
    "If you use this code, please cite:\n",
    "```bibtex\n",
    "@article{zhao2024clara,\n",
    "  title={CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning},\n",
    "  author={Zhao, Zhihao and others},\n",
    "  journal={arXiv preprint arXiv:2511.18659},\n",
    "  year={2024}\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}