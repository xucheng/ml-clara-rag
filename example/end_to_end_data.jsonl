{"question": "How do I implement gradient descent?", "docs": ["Gradient descent is an optimization algorithm that iteratively adjusts parameters to minimize a loss function by moving in the direction of steepest descent."], "gold_answer": "To implement gradient descent: 1) Initialize parameters randomly, 2) Compute loss function, 3) Calculate gradients using backpropagation, 4) Update parameters: theta = theta - learning_rate * gradient, 5) Repeat until convergence. Key considerations include choosing learning rate and batch size."}
{"question": "What are best practices for training deep learning models?", "docs": ["Training deep learning models requires careful consideration of architecture, hyperparameters, data preprocessing, and optimization strategies."], "gold_answer": "Best practices include: 1) Use appropriate data augmentation, 2) Start with a proven architecture, 3) Use batch normalization, 4) Monitor validation loss to detect overfitting, 5) Use learning rate scheduling, 6) Save checkpoints regularly, 7) Visualize training metrics, 8) Use early stopping, 9) Employ cross-validation for small datasets."}
{"question": "How do I prevent overfitting in neural networks?", "docs": ["Overfitting occurs when models memorize training data instead of learning generalizable patterns, resulting in poor performance on new data."], "gold_answer": "Methods to prevent overfitting: 1) Use more training data, 2) Apply dropout (0.2-0.5), 3) Add L1/L2 regularization, 4) Reduce model complexity, 5) Use data augmentation, 6) Implement early stopping, 7) Apply batch normalization, 8) Use ensemble methods, 9) Cross-validate your model. Monitor the gap between training and validation loss."}
{"question": "What is the difference between LSTM and GRU?", "docs": ["LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) are recurrent neural network architectures designed to handle long-term dependencies."], "gold_answer": "LSTM uses three gates (input, forget, output) and a separate cell state, while GRU combines gates into two (reset, update) with simpler architecture. GRU has fewer parameters and trains faster, while LSTM provides more control and often performs better on complex tasks. GRU is preferred for faster training, LSTM for maximum performance."}
{"question": "How do I choose batch size for training?", "docs": ["Batch size affects training dynamics, memory usage, and model generalization in deep learning."], "gold_answer": "Batch size considerations: 1) Larger batches (128-512) provide more stable gradients but require more memory, 2) Smaller batches (16-64) add noise that can help escape local minima, 3) Start with 32 or 64 as baseline, 4) Adjust based on GPU memory, 5) Use gradient accumulation for effective larger batches, 6) Consider learning rate scaling with batch size."}
{"question": "What is the purpose of learning rate scheduling?", "docs": ["Learning rate scheduling adjusts the learning rate during training to improve convergence and final model performance."], "gold_answer": "Learning rate scheduling strategies: 1) Step decay: reduce by factor every N epochs, 2) Exponential decay: continuous reduction, 3) Cosine annealing: smooth reduction following cosine curve, 4) ReduceLROnPlateau: reduce when validation loss plateaus, 5) Warmup: gradually increase initially. Proper scheduling can significantly improve final accuracy and training stability."}
{"question": "How do I implement data augmentation for images?", "docs": ["Data augmentation creates variations of training images to increase dataset diversity and improve model generalization."], "gold_answer": "Common augmentation techniques: 1) Random horizontal/vertical flips, 2) Random rotation (±15-30 degrees), 3) Random crops and resizing, 4) Color jittering (brightness, contrast, saturation), 5) Random erasing/cutout, 6) Mixup and CutMix for advanced augmentation. Libraries like torchvision.transforms or albumentations provide these transformations. Use moderate augmentation to avoid distorting data distribution."}
{"question": "What is transfer learning and when should I use it?", "docs": ["Transfer learning leverages pre-trained models to solve new tasks with limited data by reusing learned features."], "gold_answer": "Transfer learning is ideal when: 1) You have limited training data, 2) Your task is similar to the pre-training task, 3) You want faster training. Process: 1) Load pre-trained model (ResNet, BERT, etc.), 2) Freeze early layers, 3) Replace final layers for your task, 4) Fine-tune on your data. For very different tasks, unfreeze more layers. Common in computer vision and NLP."}
{"question": "How do I debug a neural network that won't train?", "docs": ["Neural networks may fail to train due to various issues including architecture problems, optimization issues, or data problems."], "gold_answer": "Debugging steps: 1) Check data: verify inputs/labels are correct, check for NaN values, 2) Simplify: try overfitting a small batch first, 3) Check loss: ensure it decreases on training set, 4) Verify gradients: check for vanishing/exploding gradients, 5) Adjust learning rate: try 1e-3, 1e-4, 1e-5, 6) Check initialization: use proper weight initialization, 7) Verify architecture: ensure forward pass works, 8) Monitor activations: check for dead ReLUs."}
{"question": "What metrics should I use to evaluate my model?", "docs": ["Model evaluation requires appropriate metrics based on the task type and business objectives."], "gold_answer": "Metric selection by task: Classification - accuracy, precision, recall, F1-score, ROC-AUC; Regression - MSE, MAE, R²; Ranking - NDCG, MAP; Consider: 1) Class imbalance: use F1 or balanced accuracy, 2) Cost-sensitive: precision for false positive costs, recall for false negatives, 3) Multi-class: macro vs micro averaging, 4) Always report multiple metrics, 5) Use confusion matrix for detailed analysis."}
