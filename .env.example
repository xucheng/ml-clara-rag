# CLaRa Environment Configuration
# Copy this file to .env and update with your values

# ===================================
# Data Pipeline Settings
# ===================================
# Directory containing raw documents (PDF, DOCX, PPTX, images)
RAW_DATA_DIR=./raw_data

# API keys for LLM services
OPENAI_API_KEY=sk-...
BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

# Model names
VISION_MODEL=qwen-vl-max    # For image description
MODEL=qwen-turbo             # For text synthesis/QA generation

# ===================================
# Training Settings
# ===================================
# Project root directory (default: current directory)
PROJECT_ROOT=$(pwd)

# Data directory
DATA_ROOT=${PROJECT_ROOT}/data
DATA_PATH=${DATA_ROOT}

# Model path (can be HuggingFace model ID or local path)
MODEL_PATH=mistralai/Mistral-7B-Instruct-v0.2
# or use local path:
# MODEL_PATH=/path/to/local/model

# Checkpoint directory
CHECKPOINT_ROOT=${PROJECT_ROOT}/checkpoints

# For Stage 2 (Instruction Tuning): Path to Stage 1 checkpoint
PRETRAIN_CKPT=${CHECKPOINT_ROOT}/clara_stage1

# For Stage 3 (End-to-End): Path to Stage 2 checkpoint
PRETRAIN_CHECKPOINT=${CHECKPOINT_ROOT}/clara_stage2

# W&B Settings
WANDB_DIR=${PROJECT_ROOT}/wandb_logs
WANDB_TOKEN=your_wandb_token_here

# ===================================
# Usage Examples
# ===================================
# 1. Run data pipeline:
#    export RAW_DATA_DIR="/path/to/your/documents"
#    bash scripts/run_data_pipeline.sh
#
# 2. Training Stage 1:
#    export MODEL_PATH="mistralai/Mistral-7B-Instruct-v0.2"
#    export CHECKPOINT_ROOT="./checkpoints"
#    bash scripts/train_pretraining.sh
#
# 3. Training Stage 2:
#    export PRETRAIN_CKPT="./checkpoints/clara_stage1"
#    bash scripts/train_instruction_tuning.sh
#
# 4. Training Stage 3:
#    export PRETRAIN_CHECKPOINT="./checkpoints/clara_stage2"
#    bash scripts/train_stage_end_to_end.sh
